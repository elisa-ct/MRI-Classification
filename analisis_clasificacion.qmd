---
title: "ANÁLISIS DE PREDICTORAS Y CLASIFICACIÓN BINARIA ENTERO"
toc: true
format:
  html:
    embed-resources: true
editor: visual
---

```{r}
#| message: false
#| warning: false

rm(list = ls())
library(GLCMTextures) # para sacar la matriz GLCM
library(imager) # para cargar las imágenes
library(tidyverse)
library(purrr)
library(tidymodels)
library(readr)
library(ggplot2)
library(corrplot)
library(performance)
library(FactoMineR)
library(factoextra)
library(ClustOfVar)
library(tidytext)
library(rpart.plot)
library(ranger)
library(caret)
library(pROC)
library(car)
library(MLmetrics)
library(vip)
library(gridExtra)
```

# 1. Análisis de las variables predictoras

Se va a realizar un análisis de variables predictoras a partir de los datasets generados. Este análisis se realiza con distintas finalidades:

-   **Reducción de la dimensionalidad**: los datasets cuentan con número elevado de variables, se tratará de comprobar si algunas de ellas pueden ser irrelevantes o redundantes para el problema de clasificación.

-   **Eliminación de multicolinealidad**: en algunos casos, las variables predictoras pueden estar altamente correlacionadas entre sí, lo que puede causar problemas en la interpretación de los coeficientes del modelo y en la estabilidad de las predicciones.

-   **Facilitar la interpretación**: al reducir la dimensionalidad del conjunto de datos, es más fácil visualizar y comprender la estructura subyacente de los datos.

-   **Mejorar el rendimiento del modelo**: al eliminar variables irrelevantes o redundantes y al reducir la multicolinealidad, los modelos de clasificación podrían mejorar su capacidad de correlación. Además, al reducir la dimensionalidad aseguramos una reducción del tiempo de ejecución en los algoritmos de clasificación.

Para llevar a cabo el análisis de las métricas, en un primer paso, se parte del *dataset* general ya que recopila todas las métricas de las distintas direcciones *shift* con las que se habían obtenido las matrices GLCM.

Los análisis que se han realizado sobre las variables predictoras son: análisis de la correlación, análisis de Componentes Principales (PCA, por sus siglas en inglés), análisis de la multicolinealidad a partir de un ajuste de regresión lineal y por último un cluster de variables.

## 1.1 Estudio de la correlación

El primer paso es estudiar la correlación entre las variables predictoras para estudiar las relaciones lineales entre ellas. Se incluye también la variable respuesta *Categoría* en el estudio de la correlación, aunque se sepa que su relación con las variables predictoras no es lineal por ser una variable dicotómica (en el caso de la clasificación binaria).

Se sabe de antemano que las métricas se clasifican en tres grupos:

1.  **Contraste**: *Contrast, Dissimilarity, Homogeneity.*

2.  **Orden**: *ASM, Entropy.*

3.  **Descriptivas**: *Mean, Variance, Correlation.*

Además de la variable *SA* que se clasifica en un grupo a parte. Se espera que exista una alta correlación entre las métricas siguiendo un patrón similar al de estos grupos.

En primer lugar se estudian las correlaciones entre las métricas de todas las direcciones sobre el *dataset* general. A partir de este análisis no solo se podrán determinar las métricas con más correlación entre sí en cada una de las direcciones, si no que también se podrá analizar la correlación que existe entre la diferentes direcciones. De esta forma se valorará si es necesario incluir todas las direcciones en el estudio, si todas ellas se comportan igual y de qué manera se correlacionan las métricas de cada una de ellas.

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv")

corr_total <- matriz_total |> 
  select(-id_img) |>
  mutate(category = category != "non_demented") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  cor()


# sin ordenar
corrplot::corrplot(corr_total, tl.cex = 0.45)
```

Aunque es complicado obtener conclusiones específicas sobre cada dirección individual, el gráfico proporciona un patrón evidente que revela información acerca del comportamiento general del conjunto de datos. Se observa que todas las métricas de todas las direcciones siguen un patrón uniforme, lo que sugiere que la elección de la dirección no afecta a la correlación entre las métricas.

A continuación, para poder visualizarlo mejor, se estudia la correlación para las métricas de una misma dirección. En este caso se ha escogido la dirección radial.

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv")
```

```{r}
corr_total <- matriz_total |> 
  select(-id_img) |>
  mutate(category = category != "non_demented") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(category, contains("r1")) |> 
  cor()

corr_total

corrplot::corrplot(corr_total, tl.cex = 0.6)
```

En el gráfico se observan tres grupos diferenciados:

1.  **Contraste**: *Contrast, Dissimilarity.*

2.  **Orden**: *Homogeneity, ASM, Entropy.*

3.  **Descriptivas**: *Mean, Variance, Correlation, SA.*

Muy similares a los grupos que se mencionaban anteriormente. También se puede observar como la variable *Correlation* presenta una alta correlación con todas las métricas. Por ello, posteriormente tanto en el PCA como en el cluster de variables se analizará si es conveniente eliminar esta variable del modelo.

Dado que todas las métricas se comportan de manera similar en todas las direcciones, el siguiente paso para reducir la dimensionalidad de los datos consiste en analizar qué dirección es la más adecuada y qué métricas específicas de esa dirección deben considerarse para los algoritmos de clasificación.

Además de la matriz de correlación se puede observar que las variables *Mean* y SA presentan una correlación igual a 1. Si se estudia el ratio entre ambas variables se obtiene que la métrica SA es igual al doble de Mean.

```{r}
ratio <- matriz_total |> 
  select(-id_img) |>
  mutate(category = category != "non_demented") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(category, contains("r1")) |> 
  select(`mean-r1`, `SA-r1`) |> 
  mutate(ratio = `SA-r1`/`mean-r1`)
```

Por ello, de aquí en adelante se eliminará la variable SA del conjunto de datos.

## 1.2 PCA

El PCA es una técnica estadística que permite reducir la dimensionalidad del conjunto de datos manteniendo la mayor cantidad posible de información. A partir del PCA se busca transformar el conjunto de métricas correlacionadas en un conjunto de variables no correlacionadas, llamadas componentes principales. Al reducir la dimensionalidad, PCA facilita la visualización y el análisis de los datos y la identificación de patrones y tendencias significativas entre las distintas métricas y las direcciones.

### 1.2.1 PCA de todas las direcciones

En un primer paso se hace PCA a partir del dataset general, considerando 45 componentes principales.

```{r}
pca_PCA <-
  PCA(matriz_total |> 
  select(-id_img, -demented, - contains("SA")), 
  scale.unit = TRUE, ncp = 45, graph = FALSE)
```

A continuación se representa la varianza que explica cada una de las componentes principales:

```{r}
# autovalores
autoval <- pca_PCA$eig
autoval2 <- get_eig(pca_PCA)

# Varianza explicada (autovalores)
fviz_eig(pca_PCA, addlabels = TRUE, barfill = "#b5ccfe", ncp = 45,
         barcolor = "#6e8ed3", xlab = "Componentes",
         ylab = "% de varianza explicada",
         main = "Varianza explicada por componentes de todas las direcciones")
```

Se observa que con tan solo tres componentes principales se explica en torno a un 98% de la varianza. Por lo tanto, se vuelve a hacer un PCA con solo tres componentes principales.

```{r}
pca_PCA <-
  PCA(matriz_total |> 
  select(-id_img, -demented, - contains("SA")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)
```

Para poder visualizar cuáles son las variables que más contribuyen a cada una de las tres componentes principales se representan las cinco variables que más contribuyen en valor absoluto a cada una de ellas:

```{r}
matriz_total <- matriz_total |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(-id_img, -demented, - contains("SA"))

corr <- pca_PCA$var$cor

coeficientes_df <- as.data.frame(corr[,1:3]) |> 
  mutate(variables = names(matriz_total))
  
# vamos a pasar las columnas a una sola pivot_longer
coeficientes_df2 <- coeficientes_df |> 
  pivot_longer(Dim.1:Dim.3, names_to = "Dim", values_to = "Coef" )

# hacemos un slice_max por componente de las 5 mejores variables
coeficientes_df2 <- coeficientes_df2 |> 
  slice_max(order_by = abs(Coef), n = 5, by = Dim) |> 
  mutate(Color = ifelse(Coef < 0, "negativo", "positivo"),
         variables = factor(variables)) |> 
  arrange(desc(abs(Coef)), by = Dim)


ggplot(coeficientes_df2) +
  geom_col(aes(x = reorder_within(variables, abs(Coef), Dim), y = abs(Coef), fill = Color)) +
  facet_wrap(~Dim, ncol = 3, scales = "free_x") +
  scale_fill_manual(values = c("positivo" = "#b5ccfe", "negativo" = "#ffc4d1")) +
  scale_x_reordered() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(title = "Métricas que más contribuyen a las componentes principales",
       subtitle = "del PCA generado a partir de todas las direcciones",
       x = "Métricas",
       y = "Correlación con la componente principal",
       fill = "Signo")
```

Las variables que más contribuyen a cada una de las componentes son:

1.  *Entropy* (la que más aporta pertenece a la dirección radial)

2.  *Variance* (aportaciones de todas las direcciones por igual)

3.  *Contrast* (las que más aportan pertenecen a las direcciones radial, c(1, -1) y c(1,1))

Cada una de ellas proviene de uno de los grupos que se obtenían en el análisis previo de correlaciones lo cual sugiere que los grupos identificados en la matriz de correlaciones tienen una relación directa con la estructura de los datos en el espacio reducido definido por las componentes principales.

-   Para clasificación binaria:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-category, - contains("SA"))
```

```{r}

matriz_total_grafico <-
  matriz_total

matriz_total_rec <- recipe(matriz_total_grafico |> select(-id_img), demented ~ .) |> 
  step_normalize(all_numeric_predictors())

matriz_total_rec <-
  matriz_total_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 3,
           prefix = "PC")

pca_tidymodels <- bake(matriz_total_rec |>  prep(), new_data = NULL)

ggplot(pca_tidymodels,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_manual(values = c("TRUE" = "#b5ccfe", "FALSE" = "#3D518C"), guide = "none") + 
    scale_fill_manual(values = c("TRUE" = "#b5ccfe", "FALSE" = "#3D518C")) +
  labs(title = "Distribución de Componentes Principales según Estado de Dementia",
         subtitle = "del PCA generado a partir del dataset de todas las direcciones",
         fill = "Demented")
```

-   Para cuatro categorías:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  rename(demented = category) |> 
  select(- contains("SA"))

```

```{r}
matriz_total_grafico <-
  matriz_total

matriz_total_rec <- recipe(matriz_total_grafico |> select(-id_img), demented ~ .) |> 
  step_normalize(all_numeric_predictors())

matriz_total_rec <-
  matriz_total_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 3,
           prefix = "PC")

pca_tidymodels <- bake(matriz_total_rec |>  prep(), new_data = NULL)

ggplot(pca_tidymodels,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
```

### 1.2.2 PCA de la dirección radial

En un primer paso se hace el PCA con nueve componentes principales:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  rename(demented = category) |> 
  select(- contains("SA"))

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 9, graph = FALSE)
```

```{r}
# autovalores
autoval <- pca_PCA1$eig
autoval2 <- get_eig(pca_PCA1)

# Varianza explicada (autovalores)
fviz_eig(pca_PCA1, addlabels = TRUE, barfill = "#b5ccfe", ncp = 9,
         barcolor = "#6e8ed3", xlab = "Componentes",
         ylab = "% de varianza explicada",
         main = "Varianza explicada por componentes dirección radial con Correlation")
```

Al igual que pasaba con el dataset general, en la dirección radial también se puede explicar en torno a un 98% de la varianza con tan solo tres componentes principales. Para poder visualizar cuáles son las variables que más contribuyen a cada una de las tres componentes principales se representan las cinco variables que más contribuyen en valor absoluto a cada una de ellas:

```{r}
pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

corr <- pca_PCA1$var$cor
```

```{r}
coeficientes_df <- as.data.frame(corr[,1:3]) |> 
  mutate(variables = names(matriz_total[35:42]))
  
# vamos a pasar las columnas a una sola pivot_longer
coeficientes_df2 <- coeficientes_df |> 
  pivot_longer(Dim.1:Dim.3, names_to = "Dim", values_to = "Coef" )

# hacemos un slice_max por componente de las 5 mejores variables
coeficientes_df2 <- coeficientes_df2 |> 
  slice_max(order_by = abs(Coef), n = 5, by = Dim) |> 
  mutate(Color = ifelse(Coef < 0, "negativo", "positivo")) 


ggplot(coeficientes_df2) +
  geom_col(aes(x = reorder_within(variables, abs(Coef), Dim), y = abs(Coef), fill = Color)) +
  facet_wrap(~Dim, ncol = 3, scales = "free_x") +
  scale_fill_manual(values = c("positivo" = "#b5ccfe", "negativo" = "#ffc4d1")) +
  scale_x_reordered() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
   labs(title = "Métricas que más contribuyen a las componentes principales",
       subtitle = "del PCA generado a partir del dataset radial con Correlation",
       x = "Métricas",
       y = "Correlación con la componente principal",
       fill = "Signo")

```

En este caso las variables que más aportan a cada una de las componentes son:

1.  *Entropy.*

2.  *Variance*.

3.  *Contrast.*

Todas son las mismas variables que se obtenía en el caso del *dataset* general.

A continuación se analizan los gráficos de dispersión de las observaciones en el espacio definido por las componentes principales según las categorías:

-   Para clasificación binaria:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-category, -contains("SA"))

matriz_total_grafico <-
  matriz_total |> 
  select(id_img, demented, contains("r1"))

matriz_total_rec <- recipe(matriz_total_grafico |> select(-id_img), demented ~ .) |> 
  step_normalize(all_numeric_predictors())

matriz_total_rec <-
  matriz_total_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 3,
           prefix = "PC")

pca_tidymodels <- bake(matriz_total_rec |>  prep(), new_data = NULL)

ggplot(pca_tidymodels,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_manual(values = c("TRUE" = "#b5ccfe", "FALSE" = "#3D518C"), guide = "none") + 
    scale_fill_manual(values = c("TRUE" = "#b5ccfe", "FALSE" = "#3D518C")) +
  labs(title = "Distribución de Componentes Principales según Estado de Dementia",
         subtitle = "del PCA generado a partir del dataset radial incluyendo Correlation",
         fill = "Demented")
```

-   Para las cuatro categorías:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  rename(demented = category) |> 
  select(-contains("SA"))

matriz_total_grafico <-
  matriz_total |> 
  select(id_img, demented, contains("r1"))

matriz_total_rec <- recipe(matriz_total_grafico |> select(-id_img), demented ~ .) |> 
  step_normalize(all_numeric_predictors())

matriz_total_rec <-
  matriz_total_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 3,
           prefix = "PC")

pca_tidymodels <- bake(matriz_total_rec |>  prep(), new_data = NULL)

ggplot(pca_tidymodels,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
```

Las conclusiones son las mismas que en el caso del dataset general. Una vez más, con ello comprobamos que la elección de la dirección radial sería una opción correcta para poder reducir la dimensionalidad de los datos.

### 1.2.3 PCA de la dirección radial sin Correlation

Anteriormente en el análisis de correlaciones se había observado que la variable *Correlation* presentaba una correlación alta con todas las métricas en todas las direcciones. Para poder comprobar si podemos eliminar *Correlation* se van a analizar los gráfico de las componentes principales del PCA sin esta variable.

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  rename(demented = category) |> 
  select(- contains("SA"), `correlation-r1`)

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 9, graph = FALSE)
```

```{r}
pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

corr <- pca_PCA1$var$cor
```

```{r}

# autovalores
autoval <- pca_PCA1$eig
autoval2 <- get_eig(pca_PCA1)

# Varianza explicada (autovalores)
fviz_eig(pca_PCA1, addlabels = TRUE, barfill = "#b5ccfe", ncp = 9,
         barcolor = "#6e8ed3", xlab = "Componentes",
         ylab = "% de varianza explicada",
         main = "Varianza explicada por componentes dirección radial")

```

```{r}
coeficientes_df <- as.data.frame(corr[,1:3]) |> 
  mutate(variables = names(matriz_total[35:42]))
  
# vamos a pasar las columnas a una sola pivot_longer
coeficientes_df2 <- coeficientes_df |> 
  pivot_longer(Dim.1:Dim.3, names_to = "Dim", values_to = "Coef" )

# hacemos un slice_max por componente de las 5 mejores variables
coeficientes_df2 <- coeficientes_df2 |> 
  slice_max(order_by = abs(Coef), n = 5, by = Dim) |> 
  mutate(Color = ifelse(Coef < 0, "negativo", "positivo")) 


ggplot(coeficientes_df2) +
  geom_col(aes(x = reorder_within(variables, abs(Coef), Dim), y = abs(Coef), fill = Color)) +
  facet_wrap(~Dim, ncol = 3, scales = "free_x") +
  scale_fill_manual(values = c("positivo" = "#b5ccfe", "negativo" = "#ffc4d1")) +
  scale_x_reordered() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
   labs(title = "Métricas que más contribuyen a las componentes principales",
       subtitle = "del PCA generado a partir del dataset radial",
       x = "Métricas",
       y = "Correlación con la componente principal",
       fill = "Signo")

```

-   Para clasificación binaria:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-category, -contains("SA"))
```

```{r}
pca_PCA1_corr <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1"), - `correlation-r1`), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)
```

```{r}

matriz_total_grafico <-
  matriz_total |> 
  select(id_img, demented, contains("r1")) |> 
  select(- `correlation-r1`)

matriz_total_rec <- recipe(matriz_total_grafico |> select(-id_img), demented ~ .) |> 
  step_normalize(all_numeric_predictors())

matriz_total_rec <-
  matriz_total_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 3,
           prefix = "PC")

pca_tidymodels <- bake(matriz_total_rec |>  prep(), new_data = NULL)

ggplot(pca_tidymodels,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_manual(values = c("TRUE" = "#b5ccfe", "FALSE" = "#3D518C"), guide = "none") + 
    scale_fill_manual(values = c("TRUE" = "#b5ccfe", "FALSE" = "#3D518C")) +
  labs(title = "Distribución de Componentes Principales según Estado de Dementia",
         subtitle = "del PCA generado a partir del dataset radial",
         fill = "Demented")
```

-   Para las cuatro categorías:

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  rename(demented = category) |> 
  select(-contains("SA"))
```

```{r}
matriz_total_grafico <-
  matriz_total |> 
  select(id_img, demented, contains("r1")) |> 
  select(- `correlation-r1`)

# matriz_total_split <- initial_split(matriz_total_grafico, strata = demented, prop = 0.8)

# matriz_total_train <- training(matriz_total_split)
# matriz_total_test <- testing(matriz_total_split)
# matriz_total_val <- vfold_cv(matriz_total_train, strata = demented, v = 5, repeats = 3)

matriz_total_rec <- recipe(data = matriz_total_grafico |> select(-id_img), demented ~ .) |> 
  step_normalize(all_numeric_predictors())

matriz_total_rec <-
  matriz_total_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 3,
           prefix = "PC")

pca_tidymodels <- bake(matriz_total_rec |>  prep(), new_data = NULL)

ggplot(pca_tidymodels,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
```

Los gráficos son muy similares a los que se obtenían en el apartado anterior, lo que nos indica que, por ahora, no tenemos resultados concluyentes para asegurar que la variable *Correlation* introduzca ruido en nuestras predicciones. En los siguientes apartados se seguirá estudiando si es conveniente o no eliminar la variable de nuestros datos.

### ANEXO: estandarización PCA

Se comprueba si la función PCA no solo estandariza por varianza si no también por media:

```{r}
data <- matriz_total |> 
  select(-id_img) |>
  select(contains("r1"))

data_scaled <- scale(data, center=TRUE, scale=TRUE)

pca_scaled <-
  PCA(data_scaled, 
  scale.unit = FALSE, ncp = 3, graph = FALSE)
```

```{r}
coord <- pca_PCA1$ind$coord
head(coord)
```

```{r}
coord_scaled <- pca_scaled$ind$coord
head(coord_scaled)
```

Los resultados son los mismos por lo que se concluye que sí lo hace.

## 1.3 Análisis Multicolinealidad

Para evaluar la multicolinealidad entre las variables predictoras en el modelo, se realizó un análisis utilizando el método de los Valores de Inflación de la Varianza (VIF) a partir de un modelo de regresión lineal múltiple. Aunque la variable de respuesta es categórica, el enfoque del análisis se centró en las relaciones entre las variables predictoras, dado que la multicolinealidad entre ellas puede afectar la precisión de las estimaciones de los coeficientes del modelo, independientemente del tipo de variable de respuesta utilizada.

Se realiza un ajuste de un modelo de regresión lineal múltiple utilizando todas las variables predictoras disponibles. Posteriormente, se calculan los VIF para cada variable y se procedió a eliminar las variables con los VIF más altos del modelo, una a una, y se repitió el proceso de ajuste del modelo y análisis de multicolinealidad. Se considera la dirección radial por los resultados obtenidos en el apartado anterior.

```{r}
#| include: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(- contains("SA"))

datos <-
  matriz_total |> 
  select(-id_img) |>
  mutate(category = category != "non_demented") |> 
  select(category, contains("r1"), - `correlation-r1`) 

# quitamos homogeneity
datos <- 
  datos |>
  select(-`homogeneity-r1`)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)

# quitamos dissimilarity
datos <- 
  datos |>
  select(-`dissimilarity-r1`)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)

# quitamos variance
datos <- 
  datos |>
  select(-`variance-r1`)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)

# quitamos glcm_correlation-c10
datos <- 
  datos |>
  select(-`correlation-r1`)
ajuste <- lm(data = datos, formula = category ~ .)
check_collinearity(ajuste)

# quitamos entropy
datos <- 
  datos |>
  select(-`entropy-r1`)
ajuste <- lm(data = datos, formula = category ~ .)
```

```{r}
#| eval: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_"))

datos <-
  matriz_total |> 
  select(-id_img) |>
  mutate(category = category != "non_demented") |> 
  select(category, contains("r1")) 

ajuste <- lm(data = datos, formula = category ~ .)
check_collinearity(ajuste)

# quitamos homogeneity
datos <- 
  datos |>
  select(-`homogeneity-r1`)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)

# quitamos dissimilarity
datos <- 
  datos |>
  select(-`dissimilarity-r1`)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)

# quitamos variance
datos <- 
  datos |>
  select(-`variance-r1`)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)

# quitamos glcm_correlation-c10
datos <- 
  datos |>
  select(-`correlation-r1`)
ajuste <- lm(data = datos, formula = category ~ .)
check_collinearity(ajuste)

# quitamos entropy
datos <- 
  datos |>
  select(-`entropy-r1`)
ajuste <- lm(data = datos, formula = category ~ .)
```

```{r}
check_collinearity(ajuste)
```

Las tres variables del ajuste final son:

1.  *Contrast.*

2.  *ASM.*

3.  *Mean.*

De nuevo, cada una pertenece a uno de los grupos definidos en el análisis de correlaciones. En este caso se ha obtenido la variable *Mean* en vez de la variable *Variance*. La consistencia en la pertenencia de las variables a los mismos grupos en diferentes análisis sugiere una estructura robusta en los datos. Esto indica que las relaciones entre las variables pueden ser estables y reproducibles, lo que aumenta la confianza en la interpretación de estos grupos como entidades distintas en el conjunto de datos. De nuevo, tenemos evidencia a favor de seleccionar una variable de cada uno de los grupos para nuestros algoritmos de clasificación y de usar las componentes principales del PCA.

Vamos a ver qué pasaría si dejamos solamente Entropy, Variance y Contrast que son las que se han elegido posteriormente como variables resumen.

```{r}
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(- contains("SA"))

datos <-
  matriz_total |> 
  select(-id_img) |>
  mutate(category = category != "non_demented") |> 
  select(category, contains("r1"), - `correlation-r1`) 

# quitamos todas menos Entropy, Variance y Contrast 
datos <- 
  datos |>
  select(`entropy-r1`, `variance-r1`, `contrast-r1`, category)
ajuste <- lm(data = datos, formula = category ~ .)

check_collinearity(ajuste)


```

Queda comprobado que también es buena opción elegir esas tres variables.

## 1.4 Análisis cluster de variables

Un cluster de variables es una técnica que agrupa variables similares en conjuntos o clusters basados en sus perfiles de correlación o covarianza. Se utilizan medidas de similitud (también conocidas como medidas de proximidad o disimilitud) para calcular la distancia entre las variables en el espacio multidimensional. Estas medidas pueden incluir la correlación, la covarianza, la distancia euclidiana, etc.

Una vez que se ha calculado la matriz de similitud entre las variables, se aplican algoritmos de clustering, como k-medias o jerárquico, para agrupar las variables en clusters basados en sus perfiles de similitud. Estos algoritmos asignan las variables a clusters de manera que las variables dentro de un mismo cluster sean más similares entre sí que con las variables de otros clusters.

### 1.4.1 Cluster de todas las direcciones

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  select(-id_img, -category, -contains("SA")) |>
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_"))

matriz_total_tree <- as.data.frame(matriz_total)

tree_todas <- hclustvar(matriz_total_tree)
plot(tree_todas)
```

Al igual que ocurría con PCA parece que se forman tres grupos diferenciados. Primero se probará este número de clusters y luego se analizará si es el adecuado. Además se analizan las variables que más correlación aportan dentro de los grupos como se hizo en PCA.

```{r}
P3_todas <- cutreevar(tree_todas, 3, matsim = TRUE)
cluster_todas <- P3_todas$cluster

P3_todas$var

cluster1 <- as.data.frame(P3_todas$var[[1]]) |> 
  mutate(cluster = "cluster1") |> 
  rownames_to_column(var = "variable") 
cluster2 <- as.data.frame(P3_todas$var[[2]]) |> 
  mutate(cluster = "cluster2") |> 
  rownames_to_column(var = "variable")
cluster3 <- as.data.frame(P3_todas$var[[3]]) |> 
  mutate(cluster = "cluster3") |> 
  rownames_to_column(var = "variable")

cluster_total <- rbind(cluster1, cluster2, cluster3) |> 
  mutate(Color = ifelse(correlation < 0, "negativo", "positivo"))

ggplot(cluster_total) +
  geom_col(aes(x = reorder(variable, abs(correlation)), y = abs(correlation), fill = Color)) +
  facet_wrap(~cluster, ncol = 3, scales = "free_x") +
  scale_fill_manual(values = c("positivo" = "#b5ccfe", "negativo" = "#ffc4d1")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(title = "Métricas que más contribuyen a los clústers",
       subtitle = "del dataset de todas las direcciones",
       x = "Variables", y = "Correlation", color = "Signo")
```

De nuevo se obtienen los mismos resultados que en los análisis anteriores. Todas las métricas de todas las direcciones se agrupan juntas, por lo que de nuevo se comprueba que la dirección escogida no influye en la relación entre las variables. Los grupos obtenidos son los mismos que en el caso del análisis de correlación. Analizamos las mejores variables grupo a grupo:

1.  *Dissimilarity* radial

2.  *Homogeneity* radial

3.  *Variance* c(0,1) c(1,0)

Podemos asegurar, una vez más, que la dirección radial es la opción correcta a escoger de todas nuestras direcciones ya que sus métricas son las que mejores resultados aportan en los dos primeros casos. En el tercer caso su valor es casi igual que el obtenido para las direcciones c(1,0) y c(0,1)

Las mejores variables no coinciden con las obtenidas en el análisis PCA excepción de *Variance*.

A continuación se hace el análisis de estabilidad. Este análisis busca determinar si los clusters identificados son sensibles a pequeñas variaciones en los datos o en los parámetros del algoritmo de clustering. En otras palabras, se trata de investigar si los clusters permanecen estables y coherentes cuando se aplican a diferentes subconjuntos de datos o se ajustan con diferentes configuraciones de análisis.

```{r}
stab <- stability(tree_todas, B = 20)
boxplot(stab$matCR, main = "Dispersion of the adjusted Rand index")
```

Se prueba el valor de cuatro clusters para ver cómo se organizarían los grupos de variables en ese caso.

```{r}
P4_todas <- cutreevar(tree_todas, 4, matsim = TRUE)
cluster_todas <- P4_todas$cluster
P4_todas$var
```

En este caso la variable *Correlation* se queda sola en un grupo.

### 1.4.2 Cluster de la dirección radial

-   Con Correlation

```{r}
#| message: false
matriz_total_r1 <- read_csv("matrices-glcm/matriz_total.csv") |> 
  select(-id_img) |>
  select(contains("r1")) |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(-`SA-r1`)

matriz_total_r1 <- as.data.frame(matriz_total_r1)
```

```{r}
tree_r1 <- hclustvar(matriz_total_r1)
plot(tree_r1)
```

De nuevo se observan tres clusters.

```{r}
P3_r1 <- cutreevar(tree_r1, 3, matsim = TRUE)
cluster_r1 <- P3_r1$cluster
P3_r1$var
```

```{r}
cluster1 <- as.data.frame(P3_r1$var[[1]]) |> 
  mutate(cluster = "cluster1") |> 
  rownames_to_column(var = "variable") 
cluster2 <- as.data.frame(P3_r1$var[[2]]) |> 
  mutate(cluster = "cluster2") |> 
  rownames_to_column(var = "variable")
cluster3 <- as.data.frame(P3_r1$var[[3]]) |> 
  mutate(cluster = "cluster3") |> 
  rownames_to_column(var = "variable")

cluster_total <- rbind(cluster1, cluster2, cluster3) |> 
  mutate(Color = ifelse(correlation < 0, "negativo", "positivo"))

ggplot(cluster_total) +
  geom_col(aes(x = reorder(variable, abs(correlation)), y = abs(correlation), fill = Color)) +
  facet_wrap(~cluster, ncol = 3, scales = "free_x") +
  scale_fill_manual(values = c("positivo" = "#b5ccfe", "negativo" = "#ffc4d1")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(title = "Métricas que más contribuyen a los clústers",
       subtitle = "del dataset de la dirección radial con Correlation",
       x = "Variables", y = "Correlation", color = "Signo")
```

Las variables que más correlación aportan dentro de cada grupo son:

1.  *Contrast/Dissimilarity*

2.  *Homogeneity*

3.  *Variance*

De nuevo se obtienen los mismos grupos. Se va a hacer un análisis de estabilidad de nuevo y las variables *Contrast* y *Variance* que se han repetido en varios de los análisis.

```{r}
stab <- stability(tree_r1, B = 40)
boxplot(stab$matCR, main = "Dispersion of the adjusted Rand index")
```

Existe evidencia a favor de considerar cuatro clusters.

```{r}
P4_r1 <- cutreevar(tree_r1, 4, matsim = TRUE)
cluster_r1 <- P4_r1$cluster
P4_r1$var
```

De nuevo, la variable *Correlation* se agrupa sola en el cuarto cluster. Considerando estos resultados y la alta correlación con todas las demás métricas observada en el análisis de correlaciones se va a plantear un algoritmo de clasificación con las componentes principales de un PCA sin incluir esta variable.

-   Sin Correlation

```{r}
#| message: false
matriz_total_r1 <- read_csv("matrices-glcm/matriz_total.csv") |> 
  select(-id_img) |>
  select(contains("r1")) |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(-`SA-r1`, - `correlation-r1`)

matriz_total_r1 <- as.data.frame(matriz_total_r1)
```

```{r}
tree_r1 <- hclustvar(matriz_total_r1)
plot(tree_r1)
```

De nuevo se observan tres clusters.

```{r}
P3_r1 <- cutreevar(tree_r1, 3, matsim = TRUE)
cluster_r1 <- P3_r1$cluster
P3_r1$var
```

```{r}
cluster1 <- as.data.frame(P3_r1$var[[1]]) |> 
  mutate(cluster = "cluster1") |> 
  rownames_to_column(var = "variable") 
cluster2 <- as.data.frame(P3_r1$var[[2]]) |> 
  mutate(cluster = "cluster2") |> 
  rownames_to_column(var = "variable")
cluster3 <- as.data.frame(P3_r1$var[[3]]) |> 
  mutate(cluster = "cluster3") |> 
  rownames_to_column(var = "variable")

cluster_total <- rbind(cluster1, cluster2, cluster3) |> 
  mutate(Color = ifelse(correlation < 0, "negativo", "positivo"))

ggplot(cluster_total) +
  geom_col(aes(x = reorder(variable, abs(correlation)), y = abs(correlation), fill = Color)) +
  facet_wrap(~cluster, ncol = 3, scales = "free_x") +
  scale_fill_manual(values = c("positivo" = "#b5ccfe", "negativo" = "#ffc4d1")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(title = "Métricas que más contribuyen a los clústers",
       subtitle = "del dataset de la dirección radial",
       x = "Variables", y = "Correlation", color = "Signo")
```

Las variables que más correlación aportan dentro de cada grupo son:

1.  *Contrast/Dissimilarity*

2.  *Homogeneity*

3.  *Variance*

Los resultados obtenidos en los distintos análisis de las variables predictoras (en el caso de la dirección radial) se resumen en la tabla siguiente, dónde se muestran las mejores variables en cada uno de ellos.

|          |                       |                        |
|----------|-----------------------|------------------------|
| **PCA**  | **Multicolinealidad** | **Cluster**            |
| Entropy  | ASM                   | Homogeneity            |
| Variance | Mean                  | Variance               |
| Contrast | Contrast              | Contrast/Dissimilarity |

Se seleccionan *Contrast* y *Mean* porque aparecen repetidas como mejores variables en varios de los análisis. En el caso del grupo de *Entropy*, *ASM* y *Homogeneity* se produce un empate entre estas variables, se selecciona *Entropy* dando prioridad a los resultados del PCA.

A continuación se va a analizar cómo se visualizan las variables en relación con las categorías, al igual que se hacía con las componentes principales del PCA.

```{r}
#| message: false

# binaria
binaria_una <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(demented, `contrast-r1`, `entropy-r1`, `variance-r1`)

# todas las categorias
cuatro_una <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  rename(demented = category) |> 
  select(demented, `contrast-r1`, `entropy-r1`, `variance-r1`)
```

```{r}
# binaria
ggplot(binaria_una,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")

# todas las categorias
ggplot(cuatro_una,
       aes(x = .panel_x, y = .panel_y,
           color = demented, fill = demented)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(vars(-demented), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
```

## 1.5 Estudio descriptivo de los datasets

#### Todas las métricas

Según indicaciones de Javi

```{r}
matriz_total_r1 <- read_csv("matrices-glcm/matriz_total.csv") |> 
  select(-id_img) |>
  select(category, contains("r1")) |> 
  mutate(category = category != "non_demented") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(-`SA-r1`) |> 
  rename_with(~ str_remove(., "-r1"), contains("r1")) |> 
  mutate(category = as.character(category)) |> 
   mutate(category = case_when(
    category == "FALSE" ~ "NON DEMENTED",
    TRUE ~ "DEMENTED"
  ))

matriz_total_r1_2 <- matriz_total_r1 |> 
  mutate(category = case_when(
    category == "NON DEMENTED" ~ "AMBAS",
    TRUE ~ "AMBAS"
  ))


matriz_r1 <- bind_rows(matriz_total_r1, matriz_total_r1_2)



matriz_r1 <- matriz_r1 |> 
  pivot_longer(`contrast`:`correlation`, names_to = "Metricas", values_to = "value_metrica") |> 
  mutate(Metricas = as_factor(Metricas)) 

```

```{r}

paleta <- c("#b3a1e3", "#91c0ee", "#9eadc7")

#CONTRAST

ggplot(matriz_r1 |> filter(Metricas == "contrast"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Contrast desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)

```

```{r}

paleta <- c("#b3a1e3", "#91c0ee", "#9eadc7")

#DISSIMILARITY

ggplot(matriz_r1 |> filter(Metricas == "dissimilarity"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Dissimilarity desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

```{r}
paleta <- c("#b3a1e3", "#91c0ee", "#9eadc7")

#HOMOGENEITY

ggplot(matriz_r1 |> filter(Metricas == "homogeneity"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Homogeneity desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

```{r}
#ASM

ggplot(matriz_r1 |> filter(Metricas == "ASM"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable ASM desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

```{r}
#Entropy

ggplot(matriz_r1 |> filter(Metricas == "entropy"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Entropy desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

```{r}
#Mean

ggplot(matriz_r1 |> filter(Metricas == "mean"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Mean desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

```{r}
# Variance

ggplot(matriz_r1 |> filter(Metricas == "variance"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Variance desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

```{r}
# Correlation

ggplot(matriz_r1 |> filter(Metricas == "correlation"), aes(x = Metricas, y = value_metrica, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Variable Correlation desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)
```

#### PCA RADIAL

```{r}
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(- contains("SA"))

pca_PCA2 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1"), -`correlation-r1`), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

PCA_r1_corr <- as.data.frame(pca_PCA2$ind$coord) |> 
  mutate(category = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img) |> 
  select(-id_img) |>
  mutate(category = as.character(category)) |> 
   mutate(category = case_when(
    category == "FALSE" ~ "NON DEMENTED",
    TRUE ~ "DEMENTED"
  ))
  
  
PCA_r1_corr_2 <- PCA_r1_corr |> 
  mutate(category = case_when(
    category == "NON DEMENTED" ~ "AMBAS",
    TRUE ~ "AMBAS"
  ))


PCA_r1 <- bind_rows(PCA_r1_corr, PCA_r1_corr_2)


PCA_r1 <- PCA_r1 |> 
  pivot_longer(`Dim.1`:`Dim.3`, names_to = "Componentes", values_to = "values") |> 
  mutate(Componentes = as_factor(Componentes)) 
  
```

```{r}

paleta <- c("#b3a1e3", "#91c0ee", "#9eadc7")

#DIM.1

ggplot(PCA_r1 |>  filter(Componentes == "Dim.1"), aes(x = Componentes, y = values, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Primera componente principal del PCA desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)

```

```{r}

paleta <- c("#b3a1e3", "#91c0ee", "#9eadc7")

#DIM.2

ggplot(PCA_r1 |>  filter(Componentes == "Dim.2"), aes(x = Componentes, y = values, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Segunda componente principal del PCA desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)

```

```{r}

paleta <- c("#b3a1e3", "#91c0ee", "#9eadc7")

#DIM.3

ggplot(PCA_r1 |>  filter(Componentes == "Dim.3"), aes(x = Componentes, y = values, fill = category)) +
  geom_boxplot(alpha = 0.9, size = 0.7, width = 0.4, notch = TRUE, outlier.alpha = 0) +  # Add geom_boxplot
  geom_jitter(aes(color = category), alpha = 0.2, size = 0.9, width = 0.2) +
  geom_violin( aes(color = category),
    alpha = 0.2, size = 0.4
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 0, 0, 0), "cm"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(face = "bold", size = 15),
        strip.text = element_text(size = 13)) +
  scale_color_manual(values = paleta) +
  scale_fill_manual(values = paleta) +
  labs(x = "Valor", title = "Tercera componente principal del PCA desagregada por categoría de demencia", fill = "Métrica") +
  facet_wrap(~category) +
  scale_x_discrete(breaks = NULL)

```

```{r}
matriz_total_r1 <- read_csv("matrices-glcm/matriz_total.csv") |> 
  select(-id_img) |>
  select(category, contains("r1")) |> 
  mutate(category = category != "non_demented") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(-`SA-r1`) |> 
  rename_with(~ str_remove(., "-r1"), contains("r1")) |> 
  mutate(category = as.character(category)) |> 
   mutate(category = case_when(
    category == "FALSE" ~ "NON DEMENTED",
    TRUE ~ "DEMENTED"
  ))

summary(matriz_total_r1 |> select(-category))
```

# 2. Clasificación binaria

Se han agrupado las cuatro categorías en dos grupos: *Non demented* y *Demented* (incluye las categorías *Very Mild, Mild* y *Moderate Demented*). Así, como un primer paso se ha realizado una clasificación binaria balanceada en la que ambos grupos contienen 3200 imágenes.

## 2.1 KNN

El algoritmo KNN (K-nearest neighbours) es un método de aprendizaje supervisado utilizado, en este caso, para clasificación. Este algoritmo se basa en la asunción de que observaciones similares en términos de las variables predictoras (los vecinos), tomarán también valores similares en la variable objetivo. Para ello, se buscan los K vecinos más cercanos (que dependen de la medida de distancia aplicada) y se calculan las probabilidades predichas mediante la frecuencia relativa de cada categoría, que puede ponderarse según la distancia anterior.

La elección del número de vecinos, el peso asignado a los vecinos cercanos (weight) y la medida de distancia utilizada afecta al rendimiento del algoritmo. Para optimizarlo, podemos formar combinaciones de los parámetros, conocido como *grid*, que incluya distintos rangos de valores para estas tres componentes. En la función elegida para ejecutar el algortimo, del paquete *tidyverse*, los valores que se pueden ajustar del *grid* son:

-   ***k (neighbors)***: representa el número de vecinos más cercanos que se considerarán al realizar una predicción.

-   ***dist (dist_power)*****:** determina la potencia de la métrica de distancia utilizada en el cálculo de la similitud entre puntos de datos. En este caso es un posible valor para el parámetro p de la distancia de Minkowski. Donde p = 2 sería la distancia Euclidiana y p = 1 sería la distancia Manhattan.

$$ D(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p} $$

-   ***weight (weight_func)***: indica la función de peso utilizada para asignar importancia a los vecinos más cercanos en función de su distancia al punto que se está clasificando.

En los algoritmos KNN que se ejecutaron, primero se realizó una partición de los datos disponibles, asignando el 80% de las imágenes para el entrenamiento del modelo y el 20% restante como datos de prueba para la evaluación. A partir de los datos de entrenamiento, se llevó a cabo un proceso de validación cruzada utilizando 5 particiones y 3 repeticiones para garantizar la robustez del modelo. Este proceso permitió evaluar múltiples configuraciones de hiperparámetros y seleccionar aquellos que produjeran el mejor rendimiento. Específicamente, se identificaron los valores óptimos de los hiperparámetros basándose en la métrica de precisión, seleccionando el modelo que maximiza esta métrica.

Con los hiperparámetros óptimos, se ajustó el modelo final utilizando la totalidad de la matriz de entrenamiento. Posteriormente, este modelo ajustado se aplicó a los datos de prueba para evaluar su rendimiento final.

### 2.1.1 Datasets

#### RADIAL Y TOTAL

Sin SA pero con Correlation

```{r}
#| message: false
matriz_r1 <- read_csv("matrices-glcm/matriz_r1.csv")
matriz_total <- read_csv("matrices-glcm/matriz_total.csv")
```

```{r}

matriz_total <-
  matriz_total |> 
  mutate(demented = category != "non_demented",
         demented = factor(demented, levels = c("TRUE", "FALSE"))) |> 
  select(-category) |> 
  select(- contains("SA"))
  

matriz_r1 <-
  matriz_total |> 
  select(contains("r1"), id_img, demented)
  
```

```{r}
# train y test
set.seed(65432)


# c(todas)
matriz_total_split <- initial_split(matriz_total, strata = demented, prop = 0.8)
matriz_total_train <- training(matriz_total_split)
matriz_total_test <- testing(matriz_total_split)

# radial
matriz_r1_split <- initial_split(matriz_r1, strata = demented, prop = 0.8)
matriz_r1_train <- training(matriz_r1_split)
matriz_r1_test <- testing(matriz_r1_split)
```

```{r}
# validation cruzada
set.seed(45678)

# c(todas)
matriz_total_val <- vfold_cv(matriz_total_train, strata = demented, v = 5, repeats = 3)

# radial
matriz_r1_val <- vfold_cv(matriz_r1_train, strata = demented, v = 5, repeats = 3)

```

```{r}
# recipe knn


# c(todas)
matriz_total_rec <- recipe(data = matriz_total_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), min = 0, max = 1) 


# radial
matriz_r1_rec <- recipe(data = matriz_r1_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), rmin = 0, max = 1)
```

```{r}
# Modelo con tune
knn_model_tune <-
  nearest_neighbor(mode = "classification", 
                   neighbors = tune("k"),
                   weight_func = tune("weight"),
                   dist_power = tune("dist")) |> 
  set_engine("kknn")
```

```{r}

# Flujo de trabajo

#c(todas)
workflow_total <- 
  workflow() |> 
  add_recipe(matriz_total_rec) |> 
  add_model(knn_model_tune)

# radial
workflow_r1 <- 
  workflow() |> 
  add_recipe(matriz_r1_rec) |> 
  add_model(knn_model_tune)
```

```{r}
grid_knn1 <- expand.grid(k = c(5, 10, 25, 50, 100, 150),
            weight = c("rectangular", "biweight", "inv"),
            dist = c(0.1, 1, 2, 10, 20))

```

```{r}
#| eval: false
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, 
                         roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_knn1
                

#c(todas)
total_knn_fit_tune <-
  workflow_total |> 
  tune_grid(resamples = matriz_total_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)


# radial
r1_knn_fit_tune <-
  workflow_r1 |> 
  tune_grid(resamples = matriz_r1_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)
```

```{r}
#| eval: false

metrics_KNN_total <- list(total_knn_fit_tune_saved = total_knn_fit_tune)
metrics_KNN_radial <- list(r1_knn_fit_tune_saved = r1_knn_fit_tune)

save(metrics_KNN_total, file = "./metricas/metrics_KNN_total.RData")
save(metrics_KNN_radial, file = "./metricas/metrics_KNN_radial.RData")
```

```{r}
metrics_KNN_total <- list(total_knn_fit_tune_saved = total_knn_fit_tune)

save(metrics_KNN_total, file = "./metricas/metrics_KNN_total.RData")
```

```{r}

load("./metricas/metrics_KNN_total.RData")
load("./metricas/metrics_KNN_radial.RData")
total_knn_fit_tune <- metrics_KNN_total$total_knn_fit_tune_saved
r1_knn_fit_tune <- metrics_KNN_radial$r1_knn_fit_tune_saved
```

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}

# general
best_models <- collect_metrics(total_knn_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_knn_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))

# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN de la validación cruzada", subtitle = "obtenidos a partir del dataset total")


# radial
best_models <- collect_metrics(r1_knn_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_knn_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))

# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN de la validación cruzada", subtitle = "obtenidos a partir del dataset radial")
```

```{r}

# c(todas)
best_total_knn <- total_knn_fit_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy")

final_wf_total <- 
  workflow_total |> 
  finalize_workflow(best_total_knn)

final_knn_fit_total <- 
  final_wf_total |> 
  last_fit(matriz_total_split)

```

```{r}
# radial
best_r1_knn <- r1_knn_fit_tune |> 
  select_by_one_std_err(desc(k), metric = "accuracy")

final_wf_r1 <- 
  workflow_r1 |> 
  finalize_workflow(best_r1_knn)

final_knn_fit_r1 <- 
  final_wf_r1 |> 
  last_fit(matriz_r1_split)
```

**Resultados de la clasificación Total.**

Para validación

```{r}
best_model <- total_knn_fit_tune |> 
  select_by_one_std_err(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_knn_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)


colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para dataset total",
       subtitle = "Modelo 25k-biweight-dist10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_knn_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_total<- augment(extract_workflow(final_knn_fit_total), matriz_total_train)

conf_mat_train <-
  prob_train_total |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_total <- augment(extract_workflow(final_knn_fit_total), matriz_total_test)

conf_mat_test <-
  prob_test_total |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total$demented, prob_train_total$.pred_TRUE)
roc_obj_test <- roc(prob_test_total$demented, prob_test_total$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_KNN_total <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_total, file = "./metricas/results_KNN_total.RData")
```

**Resultados de la clasificación Radial**

Para validación

```{r}
best_model <- r1_knn_fit_tune |> 
  select_by_one_std_err(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- r1_knn_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para dataset radial",
       subtitle = "Modelo 50k-biweight-dist10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(r1_knn_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_r1<- augment(extract_workflow(final_knn_fit_r1), matriz_r1_train)

conf_mat_train <-
  prob_train_r1 |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_r1  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_r1  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_r1 <- augment(extract_workflow(final_knn_fit_r1), matriz_r1_test)

conf_mat_test <-
  prob_test_r1 |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_r1  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_r1  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_r1$demented, prob_train_r1$.pred_TRUE)
roc_obj_test <- roc(prob_test_r1$demented, prob_test_r1$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_KNN_radial <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_radial, file = "./metricas/results_KNN_radial.RData")
```

### 2.1.1 Clasificación con PCA

Para mejorar la eficiencia y la precisión de nuestro algoritmo KNN, se ha aplicado un análisis de componentes principales *dataset* de la dirección radial.

#### RADIAL con Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(- contains("SA"))

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

PCA_r1 <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)


```

```{r}
# Partición train y test

set.seed(45678)
pca_r1_split <- initial_split(PCA_r1, strata = demented, prop = 0.8)
pca_r1_train <- training(pca_r1_split)
pca_r1_test <- testing(pca_r1_split)
```

```{r}
# validation cruzada
set.seed(45678)

pca_r1_val <- vfold_cv(pca_r1_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
pca_r1_rec <- recipe(data = pca_r1_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), rmin = 0, max = 1)
```

```{r}
# Modelo con tune
knn_model_tune <-
  nearest_neighbor(mode = "classification", 
                   neighbors = tune("k"),
                   weight_func = tune("weight"),
                   dist_power = tune("dist")) |> 
  set_engine("kknn")
```

```{r}
# Flujo de trabajo
# radial
workflow_r1 <- 
  workflow() |> 
  add_recipe(pca_r1_rec) |> 
  add_model(knn_model_tune)
```

```{r}
# Grid
grid_knn1 <- expand.grid(k = c(25, 100, 175, 250, 325, 400, 500),
            weight = c("rectangular", "biweight", "inv"),
            dist = c(0.1, 1, 2, 10, 20))

```

```{r}
#| eval: false

# Evaluación en validación


# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, 
                         roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_knn1
                

r1_knn_pca_tune <-
  workflow_r1 |> 
  tune_grid(resamples = pca_r1_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)

```

```{r}
#| eval: false
# Guardar métricas
metrics_KNN_r1_PCA_corr <- list(r1_knn_fit_tune_saved = r1_knn_pca_tune)

# Guardar los resultados en un archivo .rdata
save(metrics_KNN_r1_PCA_corr, file = "./metricas/metrics_KNN_r1_PCA_corr.RData")
```

```{r}

load("./metricas/metrics_KNN_r1_PCA_corr.RData")
r1_knn_pca_tune <- metrics_KNN_r1_PCA_corr$r1_knn_fit_tune_saved
```

**Gráfico mejores modelos de la validación cruzada.**

Con el objetivo de evaluar la precisión de las diferentes combinaciones presentes en el *grid*, se ha generado un gráfico que ilustra la precisión obtenida por los 30 modelos mejor clasificados durante el proceso de validación cruzada. Este gráfico permite visualizar de manera clara y comparativa el rendimiento de las distintas configuraciones de parámetros del modelo, ayudando a identificar las combinaciones más efectivas y prometedoras. Luego se selecciona el mejor de los modelos obtenidos para obtener los hiperparámetros óptimos y ajustar el modelo final.

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}
best_models <- collect_metrics(r1_knn_pca_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- r1_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))

# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN", subtitle = "obtenidos a partir del PCA de la dirección radial incluyendo Correlation")
```

Para ver cuál ha sido el mejor de los modelos se selecciona el modelo con el mayor valor del hiperparámetro *k* (número de vecinos) cuyo desempeño en precisión (accuracy) está dentro de una desviación estándar del mejor desempeño observado.

El mejor modelo obtenido es el *200k-biweight-dist1*. Es decir, el mejor modelo se ha dado para un número de vecinos igual a 200, una ponderación *biweigth* y la distancia de Manhattan.

```{r}
# Selección del mejor

# radial
best_r1_knn_pca <- r1_knn_pca_tune |> 
  select_by_pct_loss(metric = "accuracy", desc(k))



final_wf_r1_pca <- 
  workflow_r1 |> 
  finalize_workflow(best_r1_knn_pca)

final_knn_pca_r1 <- 
  final_wf_r1_pca |> 
  last_fit(pca_r1_split)
```

**Resultados de la clasificación.**

Para validación

```{r}
best_model <- r1_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- r1_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para PCA",
       subtitle = "Modelo 500k-rectangular-dist1",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(r1_knn_pca_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_r1_pca <- augment(extract_workflow(final_knn_pca_r1), pca_r1_train)

conf_mat_train <-
  prob_train_r1_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_r1_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_r1_pca  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_r1_pca <- augment(extract_workflow(final_knn_pca_r1), pca_r1_test)

conf_mat_test <-
  prob_test_r1_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_r1_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_r1_pca  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_r1_pca$demented, prob_train_r1_pca$.pred_TRUE)
roc_obj_test <- roc(prob_test_r1_pca$demented, prob_test_r1_pca$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_KNN_r1_PCA_corr <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_r1_PCA_corr, file = "./metricas/results_KNN_r1_PCA_corr.RData")
```

#### RADIAL sin Correlation

A continuación se repite el procedimiento pero con las componentes principales del PCA de la dirección radial que no incluye la variable *Correlation*.

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(- contains("SA"))

pca_PCA2 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1"), -`correlation-r1`), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

PCA_r1_corr <- as.data.frame(pca_PCA2$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# Partición train y test
set.seed(45678)
pca_r1_corr_split <- initial_split(PCA_r1_corr, strata = demented, prop = 0.8)
pca_r1_corr_train <- training(pca_r1_corr_split)
pca_r1_corr_test <- testing(pca_r1_corr_split)
```

```{r}
# Validación cruzada
set.seed(45678)
pca_r1_corr_val <- vfold_cv(pca_r1_corr_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
pca_r1_corr_rec <- recipe(data = pca_r1_corr_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), rmin = 0, max = 1)
```

```{r}
# Modelo con tune
knn_model_tune <-
  nearest_neighbor(mode = "classification", 
                   neighbors = tune("k"),
                   weight_func = tune("weight"),
                   dist_power = tune("dist")) |> 
  set_engine("kknn")
```

```{r}
# Flujo de trabajo
workflow_r1_corr <- 
  workflow() |> 
  add_recipe(pca_r1_corr_rec) |> 
  add_model(knn_model_tune)
```

```{r}
# Grid
grid_knn1 <- expand.grid(k = c(25, 100, 175, 250, 325, 400, 500),
            weight = c("rectangular", "biweight", "inv"),
            dist = c(0.1, 1, 2, 10, 20))

```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, 
                         roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_knn1
                

r1_corr_knn_pca_tune <-
  workflow_r1_corr |> 
  tune_grid(resamples = pca_r1_corr_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)
```

```{r}
#| eval: false
# Guardar métricas
metrics_KNN_r1_PCA <- list(r1_corr_knn_fit_tune_saved = r1_corr_knn_pca_tune)

save(metrics_KNN_r1_PCA, file = "./metricas/metrics_KNN_r1_PCA.RData")
```

```{r}

load("./metricas/metrics_KNN_r1_PCA.RData")
r1_corr_knn_pca_tune <- metrics_KNN_r1_PCA$r1_corr_knn_fit_tune_saved
```

**Mejores modelos de la validación cruzada.**

```{r}
# guardo los resultados de la validacion
metrics_val_knn_pca <- collect_metrics(r1_corr_knn_pca_tune)

# Guardar la tabla como un archivo CSV
write.csv(metrics_val_knn_pca, "./tablas_val/metrics_val_knn_pca.csv", row.names = FALSE)
```

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}
best_models <- collect_metrics(r1_corr_knn_pca_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- r1_corr_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN", subtitle = "obtenidos a partir del PCA de la dirección radial")




```

El mejor modelo obtenido en este caso es el *200k-inv-dist1*. Es decir, el mejor modelo se ha dado para un número de vecinos igual a 200, una ponderación *inv* y la distancia de *Manhattan*.

```{r}
# Selección del mejor
# radial pca sin correlation
best_r1_knn_pca_corr <- r1_corr_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy")

# "Preprocessor1_Model065" "Preprocessor1_Model058" "Preprocessor1_Model045" "Preprocessor1_Model047" "Preprocessor1_Model044" "Preprocessor1_Model009" "Preprocessor1_Model051"  "Preprocessor1_Model046" "Preprocessor1_Model066" "Preprocessor1_Model048" "Preprocessor1_Model052" "Preprocessor1_Model059"  "Preprocessor1_Model067"


# best_r1_knn_pca_corr <- collect_metrics(r1_corr_knn_pca_tune) |> filter(.metric == "accuracy" & .config == "Preprocessor1_Model067")

final_wf_r1_pca_corr <- 
  workflow_r1_corr |> 
  finalize_workflow(best_r1_knn_pca_corr)

final_knn_pca_r1_corr <- 
  final_wf_r1_pca_corr |> 
  last_fit(pca_r1_corr_split)
```

**Resultados de la clasificación.**

Para validación

```{r}
best_model <- r1_corr_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- r1_corr_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para PCA",
       subtitle = "Modelo 500k-rectangular-dist1",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(r1_corr_knn_pca_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_r1_pca_corr <- augment(extract_workflow(final_knn_pca_r1_corr), pca_r1_corr_train)

conf_mat_train <-
  prob_train_r1_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_r1_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_r1_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_r1_pca_corr <- augment(extract_workflow(final_knn_pca_r1_corr), pca_r1_corr_test)

conf_mat_test <-
  prob_test_r1_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_r1_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_r1_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_r1_pca_corr$demented, prob_train_r1_pca_corr$.pred_TRUE)
roc_obj_test <- roc(prob_test_r1_pca_corr$demented, prob_test_r1_pca_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_KNN_r1_PCA <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_r1_PCA, file = "./metricas/results_KNN_r1_PCA.RData")
```

#### TOTAL con Correlation

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(- contains("SA"))

pca_PCA2 <-
  PCA(matriz_total |> 
  select(-id_img, -category),
  scale.unit = TRUE, ncp = 3, graph = FALSE)

PCA_total <- as.data.frame(pca_PCA2$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# Partición train y test
set.seed(45678)
pca_total_split <- initial_split(PCA_total, strata = demented, prop = 0.8)
pca_total_train <- training(pca_total_split)
pca_total_test <- testing(pca_total_split)
```

```{r}
# Validación cruzada
set.seed(45678)
pca_total_val <- vfold_cv(pca_total_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
pca_total_rec <- recipe(data = pca_total_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), rmin = 0, max = 1)
```

```{r}
# Modelo con tune
knn_model_tune <-
  nearest_neighbor(mode = "classification", 
                   neighbors = tune("k"),
                   weight_func = tune("weight"),
                   dist_power = tune("dist")) |> 
  set_engine("kknn")
```

```{r}
# Flujo de trabajo
workflow_total <- 
  workflow() |> 
  add_recipe(pca_total_rec) |> 
  add_model(knn_model_tune)
```

```{r}
# Grid
grid_knn1 <- expand.grid(k = c(25, 100, 175, 250, 325, 400, 500),
            weight = c("rectangular", "biweight", "inv"),
            dist = c(0.1, 1, 2, 10, 20))

```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, 
                         roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_knn1
                

total_knn_pca_tune <-
  workflow_total |> 
  tune_grid(resamples = pca_total_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)
```

```{r}
#| eval: false
# Guardar métricas
metrics_KNN_PCA_corr_total <- list(total_knn_fit_tune_saved = total_knn_pca_tune)

save(metrics_KNN_PCA_corr_total, file = "./metricas/metrics_KNN_PCA_corr_total.RData")
```

```{r}

load("./metricas/metrics_KNN_PCA_corr_total.RData")
total_knn_pca_tune <- metrics_KNN_PCA_corr_total$total_knn_fit_tune_saved
```

**Mejores modelos de la validación cruzada.**

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}
best_models <- collect_metrics(total_knn_pca_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN", subtitle = "obtenidos a partir del PCA de todas las direcciones incluyendo Correlation")

```

El mejor modelo obtenido en este caso es el *200k-inv-dist1*. Es decir, el mejor modelo se ha dado para un número de vecinos igual a 200, una ponderación *inv* y la distancia de *Manhattan*.

```{r}
# Selección del mejor
# radial pca sin correlation
best_total_knn_pca <- total_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy")

final_wf_total_pca <- 
  workflow_total |> 
  finalize_workflow(best_total_knn_pca)

final_knn_pca_total <- 
  final_wf_total_pca |> 
  last_fit(pca_total_split)
```

**Resultados de la clasificación.**

Para validación

```{r}
best_model <- total_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para PCA todas las direcciones incluyendo Correlation",
       subtitle = "Modelo 500k-rectangular-dist1",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_knn_pca_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_total_pca <- augment(extract_workflow(final_knn_pca_total), pca_total_train)

conf_mat_train <-
  prob_train_total_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total_pca |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_total_pca <- augment(extract_workflow(final_knn_pca_total), pca_total_test)

conf_mat_test <-
  prob_test_total_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total_pca  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total_pca$demented, prob_train_total_pca$.pred_TRUE)
roc_obj_test <- roc(prob_test_total_pca$demented, prob_test_total_pca$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_KNN_total_PCA_corr <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_total_PCA_corr, file = "./metricas/results_KNN_total_PCA_corr.RData")
```

#### TOTAL sin Correlation

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(- contains("SA"))

pca_PCA2 <-
  PCA(matriz_total |> 
  select(-id_img, -category) |>
  select(-contains("correlation")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

PCA_total_corr <- as.data.frame(pca_PCA2$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# Partición train y test
set.seed(45678)
pca_total_corr_split <- initial_split(PCA_total_corr, strata = demented, prop = 0.8)
pca_total_corr_train <- training(pca_total_corr_split)
pca_total_corr_test <- testing(pca_total_corr_split)
```

```{r}
# Validación cruzada
set.seed(45678)
pca_total_corr_val <- vfold_cv(pca_total_corr_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
pca_total_corr_rec <- recipe(data = pca_total_corr_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), rmin = 0, max = 1)
```

```{r}
# Modelo con tune
knn_model_tune <-
  nearest_neighbor(mode = "classification", 
                   neighbors = tune("k"),
                   weight_func = tune("weight"),
                   dist_power = tune("dist")) |> 
  set_engine("kknn")
```

```{r}
# Flujo de trabajo
workflow_total_corr <- 
  workflow() |> 
  add_recipe(pca_total_corr_rec) |> 
  add_model(knn_model_tune)
```

```{r}
# Grid
grid_knn1 <- expand.grid(k = c(25, 100, 175, 250, 325, 400, 500),
            weight = c("rectangular", "biweight", "inv"),
            dist = c(0.1, 1, 2, 10, 20))

```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, 
                         roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_knn1
                

total_corr_knn_pca_tune <-
  workflow_total_corr |> 
  tune_grid(resamples = pca_total_corr_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)
```

```{r}
#| eval: false
# Guardar métricas
metrics_KNN_PCA_total <- list(total_corr_knn_fit_tune_saved = total_corr_knn_pca_tune)

save(metrics_KNN_PCA_total, file = "./metricas/metrics_KNN_PCA_total.RData")
```

```{r}

load("./metricas/metrics_KNN_PCA_total.RData")
total_corr_knn_pca_tune <- metrics_KNN_PCA_total$total_corr_knn_fit_tune_saved
```

**Mejores modelos de la validación cruzada.**

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}
best_models <- collect_metrics(total_corr_knn_pca_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_corr_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN", subtitle = "obtenidos a partir del PCA de todas las direcciones")

```

El mejor modelo obtenido en este caso es el *200k-inv-dist1*. Es decir, el mejor modelo se ha dado para un número de vecinos igual a 200, una ponderación *inv* y la distancia de *Manhattan*.

```{r}
# Selección del mejor
# radial pca sin correlation
best_total_knn_pca_corr <- total_corr_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy")

final_wf_total_pca_corr <- 
  workflow_total_corr |> 
  finalize_workflow(best_total_knn_pca_corr)

final_knn_pca_total_corr <- 
  final_wf_total_pca_corr |> 
  last_fit(pca_total_corr_split)
```

**Resultados de la clasificación.**

Para validación

```{r}
best_model <- total_corr_knn_pca_tune |> 
  select_by_pct_loss(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_corr_knn_pca_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para PCA todas las direcciones",
       subtitle = "Modelo 500k-rectangular-dist1",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_corr_knn_pca_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)

```

Para train

```{r}
prob_train_total_pca_corr <- augment(extract_workflow(final_knn_pca_total_corr), pca_total_corr_train)

conf_mat_train <-
  prob_train_total_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_total_pca_corr <- augment(extract_workflow(final_knn_pca_total_corr), pca_total_corr_test)

conf_mat_test <-
  prob_test_total_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total_pca_corr$demented, prob_train_total_pca_corr$.pred_TRUE)
roc_obj_test <- roc(prob_test_total_pca_corr$demented, prob_test_total_pca_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black")
plot(roc_obj_test, add = TRUE, col = "red")
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_KNN_total_PCA <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_total_PCA, file = "./metricas/results_KNN_total_PCA.RData")
```

### 2.1.2 Clasificación con variables

En el análisis de las predictoras se concluyó que las métricas se agrupaban en tres grupos diferentes y que la variable *Correlation* podía ir sola en un grupo a parte. Se estudió qué variable de cada uno de los grupos elegir para poder aplicar una variable por grupo en los algoritmos de clasificación.

Como se comentaba las variables seleccionadas para esta clasificación son: *Contrast*, *Entropy* y *Variance*.

```{r}
#| message: false
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented")

radial_corr <-
  matriz_total |> 
  select(demented, "contrast-r1", "entropy-r1", "variance-r1") |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE")),
         id_img = matriz_total$id_img) 

```

```{r}
# Partición train y test
set.seed(45678)
corr_r1_split <- initial_split(radial_corr, strata = demented, prop = 0.8)
corr_r1_train <- training(corr_r1_split)
corr_r1_test <- testing(corr_r1_split)
```

```{r}
# validation cruzada
set.seed(45678)

corr_r1_val <- vfold_cv(corr_r1_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
corr_r1_rec <- recipe(data = corr_r1_train |> select(- id_img), demented ~ .) |> 
  step_range(all_numeric_predictors(), rmin = 0, max = 1) 
```

```{r}
# Modelo con tune
knn_model_tune <-
  nearest_neighbor(mode = "classification", 
                   neighbors = tune("k"),
                   weight_func = tune("weight"),
                   dist_power = tune("dist")) |> 
  set_engine("kknn")
```

```{r}
# Flujo de trabajo
# radial
workflow_r1 <- 
  workflow() |> 
  add_recipe(corr_r1_rec) |> 
  add_model(knn_model_tune)
```

```{r}
# Grid
grid_knn1 <- expand.grid(k = c(25, 50, 100, 150, 200, 300, 400),
            weight = c("rectangular", "biweight", "inv"),
            dist = c(0.1, 1, 2, 10, 20))

```

```{r}
#| eval: false
# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_knn1
                

r1_knn_corr_tune <-
  workflow_r1 |> 
  tune_grid(resamples = corr_r1_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)
```

```{r}
#| eval: false

# Guardar métricas
metrics_KNN_r1_var <- list(r1_knn_fit_tune_saved = r1_knn_corr_tune)

# Guardar los resultados en un archivo .rdata
save(metrics_KNN_r1_var, file = "./metricas/metrics_KNN_r1_var.RData")
```

```{r}

load("./metricas/metrics_KNN_r1_var.RData")
r1_knn_corr_tune <- metrics_KNN_r1_var$r1_knn_fit_tune_saved
```

**Gráfico mejores modelos de la validación cruzada.**

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}
best_models <- collect_metrics(r1_knn_corr_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- r1_knn_corr_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(k, weight, dist, .estimate) |> 
  mutate(model = glue::glue("{k}k-{weight}-dist{dist}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos KNN", subtitle = "obtenidos a partir de: Contrast, Entropy y Variance")

```

```{r}
# Selección del mejor
# radial
best_r1_knn_corr <- r1_knn_corr_tune |> 
  select_by_one_std_err(desc(k), metric = "accuracy")

final_wf_r1_corr <- 
  workflow_r1 |> 
  finalize_workflow(best_r1_knn_corr)

final_knn_corr_r1 <- 
  final_wf_r1_corr |> 
  last_fit(corr_r1_split)
```

**Resultados de la clasificación.**

Para validacion

```{r}
best_model <- r1_knn_corr_tune |> 
  select_by_one_std_err(desc(k), metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- r1_knn_corr_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN a partir de las variables: Contrast, Entropy y Variance",
       subtitle = "Modelo 200k-biweight-dist2",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")


metrics_val <- collect_metrics(r1_knn_corr_tune)|> 
  filter(.config == best_model) |> 
  select(.metric, mean, std_err)

metrics_val
```

Para train

```{r}
prob_train_r1_corr <- augment(extract_workflow(final_knn_corr_r1), corr_r1_train)

conf_mat_train <-
  prob_train_r1_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_r1_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_r1_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_r1_corr <- augment(extract_workflow(final_knn_corr_r1), corr_r1_test)

conf_mat_test <-
  prob_test_r1_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_r1_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_r1_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_r1_corr$demented, prob_train_r1_corr$.pred_TRUE)
roc_obj_test <- roc(prob_test_r1_corr$demented, prob_test_r1_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
#| eval: false

results_KNN_r1_var <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_KNN_r1_var, file = "./metricas/results_KNN_r1_var.RData")
```

## 2.2 Árbol de decisión

El segundo algoritmo de clasificación aplicado al conjunto de datos es el árbol de decisión. Este algoritmo recibe su nombre debido a su estructura que adopta la forma de un árbol.

El árbol de decisión opera mediante un proceso iterativo de selección y división de variables predictoras para construir un modelo de clasificación en nuestro caso. Comienza en un nodo raíz y selecciona la variable que mejor divide los datos en subconjuntos homogéneos. Luego, repite este proceso en cada nodo interno, creando ramas que representan las diferentes condiciones de las variables. El árbol crece hasta que se alcanza un criterio de parada predefinido, como la profundidad máxima o el número mínimo de observaciones en un nodo. Posteriormente, se puede aplicar una técnica de poda para evitar el sobreajuste.

Al igual que se procedía con el KNN, en el árbol de decisión también se contruye un grid de parámetros de los que dependerá el rendimiento del modelo. Estos parámetros son los siguientes:

-   ***tree_depth***: controla la profundidad máxima del árbol, lo que limita el número de divisiones que puede tener el árbol.

-   ***min_n***: establece el número mínimo de observaciones que deben existir en un nodo para que se realice una división adicional.

-   ***cost_complexity***: controla la complejidad del árbol de decisión. Este parámetro se utiliza para la poda del árbol y ajusta la tasa de crecimiento del árbol en función de la complejidad del modelo.

Al igual que en KNN se ha hecho una partición, el 80% de las imágenes se usan para entrenar y el 20% como datos de prueba para evaluar el modelo. También se han elegido los hiperparámetros óptimos mediante validación cruzada aplicada en los datos de entrenamiento y finalmente se han obtenido los resultados finales de aplicar el modelo ajustado a los datos de prueba.

### 2.2.1 Datasets

### RADIAL Y TOTAL

```{r}
#| message: false
matriz_r1 <- read_csv("matrices-glcm/matriz_r1.csv")
matriz_total <- read_csv("matrices-glcm/matriz_total.csv")
```

```{r}

matriz_total <-
  matriz_total |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented",
         demented = factor(demented, levels = c("TRUE", "FALSE"))) |> 
  select(-category) |> 
  select(- contains("SA"))
  

matriz_r1 <-
  matriz_total |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  select(contains("r1"), id_img, demented)
  
```

```{r}
# train y test
set.seed(65432)


# c(todas)
matriz_total_split <- initial_split(matriz_total, strata = demented, prop = 0.8)
matriz_total_train <- training(matriz_total_split)
matriz_total_test <- testing(matriz_total_split)

# radial
matriz_r1_split <- initial_split(matriz_r1, strata = demented, prop = 0.8)
matriz_r1_train <- training(matriz_r1_split)
matriz_r1_test <- testing(matriz_r1_split)
```

```{r}
# validation cruzada
set.seed(45678)

# c(todas)
matriz_total_val <- vfold_cv(matriz_total_train, strata = demented, v = 5, repeats = 3)

# radial
matriz_r1_val <- vfold_cv(matriz_r1_train, strata = demented, v = 5, repeats = 3)

```

```{r}
# recipe knn

# c(todas)
matriz_total_rec <- recipe(data = matriz_total_train |> select(- id_img), demented ~ .) 


# radial
matriz_r1_rec <- recipe(data = matriz_r1_train |> select(- id_img), demented ~ .) 
```

```{r}
# Modelo 
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))

decision_tree_gini <- decision_tree |>  set_engine("rpart") 
decision_tree_entropy <- decision_tree |>  set_engine("C5.0")
```

```{r}

# Flujo de trabajo

#c(todas)
workflow_total <- 
  workflow() |> 
  add_recipe(matriz_total_rec) |> 
  add_model(decision_tree_gini)

# radial
workflow_r1 <- 
  workflow() |> 
  add_recipe(matriz_r1_rec) |> 
  add_model(decision_tree_gini)
```

```{r}
# Grid
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4),
              "min_n" = c(10, 25, 50, 100),
              "cost" = c(0.001, 0.01, 0.1, 1))

```

```{r}
#| eval: false
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, 
                         roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_tree
                

#c(todas)
total_arbol_fit_tune <-
  workflow_total |> 
  tune_grid(resamples = matriz_total_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)


# radial
r1_arbol_fit_tune <-
  workflow_r1 |> 
  tune_grid(resamples = matriz_r1_val,
            grid = grid_select,
            control =
            control_grid(verbose = TRUE),
            metrics = metricas)
```

```{r}
#| eval: false

metrics_arbol_total <- list(total_arbol_fit_tune_saved = total_arbol_fit_tune)
metrics_arbol_radial <- list(r1_arbol_fit_tune_saved = r1_arbol_fit_tune)

save(metrics_arbol_total, file = "./metricas/metrics_arbol_total.RData")
save(metrics_arbol_radial, file = "./metricas/metrics_arbol_radial.RData")
```

```{r}

load("./metricas/metrics_arbol_total.RData")
load("./metricas/metrics_arbol_radial.RData")
total_arbol_fit_tune <- metrics_arbol_total$total_arbol_fit_tune_saved
r1_arbol_fit_tune <- metrics_arbol_radial$r1_arbol_fit_tune_saved
```

```{r}
# pca
total_arbol_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + 
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del dataset total")

r1_arbol_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + 
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del dataset radial")

```

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",
             "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",
              "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",
              "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",
              "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}

# general
best_models <- collect_metrics(total_arbol_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_arbol_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir del dataset total")


# radial
best_models <- collect_metrics(r1_arbol_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- r1_arbol_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir del dataset radial")
```

```{r}

# c(todas)
best_total_arbol <- total_arbol_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_total <- 
  workflow_total |> 
  finalize_workflow(best_total_arbol)

final_arbol_fit_total <- 
  final_wf_total |> 
  last_fit(matriz_total_split)

# radial
best_r1_arbol <- r1_arbol_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_r1 <- 
  workflow_r1 |> 
  finalize_workflow(best_r1_arbol)

final_knn_fit_r1 <- 
  final_wf_r1 |> 
  last_fit(matriz_r1_split)

```

**Resultados de la clasificación Total.**

Para validación

```{r}
best_model <- total_arbol_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_arbol_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)


colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo Árbol para dataset total",
       subtitle = "Modelo 0.001cost-3depth-min_n10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_arbol_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_total<- augment(extract_workflow(final_arbol_fit_total), matriz_total_train)

conf_mat_train <-
  prob_train_total |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_total <- augment(extract_workflow(final_arbol_fit_total), matriz_total_test)

conf_mat_test <-
  prob_test_total |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total$demented, prob_train_total$.pred_TRUE)
roc_obj_test <- roc(prob_test_total$demented, prob_test_total$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_arbol_total <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_total, file = "./metricas/results_arbol_total.RData")
```

```{r}
final_arbol_fit_total |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión del dataset de todas las direcciones") 
```

**Resultados de la clasificación Radial**

Para validación

```{r}
best_model <- r1_arbol_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- r1_arbol_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo KNN para dataset radial",
       subtitle = "Modelo 0.001cost-4depth-min_n10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(r1_arbol_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_r1<- augment(extract_workflow(final_knn_fit_r1), matriz_r1_train)

conf_mat_train <-
  prob_train_r1 |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_r1  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_r1  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
prob_test_r1 <- augment(extract_workflow(final_knn_fit_r1), matriz_r1_test)

conf_mat_test <-
  prob_test_r1 |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_r1  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_r1  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_r1$demented, prob_train_r1$.pred_TRUE)
roc_obj_test <- roc(prob_test_r1$demented, prob_test_r1$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
# Guardar resultados
# Crear una lista para almacenar los datos
results_arbol_radial <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_radial, file = "./metricas/results_arbol_radial.RData")
```

```{r}

final_knn_fit_r1 |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión del dataset radial") 
```

### 2.2.2 Clasificación con PCA

Se sigue el mismo procedimiento que con el KNN. Primero se realizará la clasificación con las componentes principales del PCA que incluye la variable *Correlation* y en segundo lugar con el PCA que no la incluye.

#### RADIAL con Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

radial_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$demented, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# train y test
set.seed(65432)

# pca
radial_pca_split <- initial_split(radial_pca, strata = demented, prop = 0.8)
radial_pca_train <- training(radial_pca_split)
radial_pca_test <- testing(radial_pca_split)
```

```{r}
# validation cruzada
set.seed(45678)

# pca
radial_pca_val <- vfold_cv(radial_pca_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
# pca
radial_pca_rec <- recipe(data = radial_pca_train |> select(- id_img), demented ~ .) 
```

```{r}
# Modelo 
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))

decision_tree_gini <- decision_tree |>  set_engine("rpart") 
decision_tree_entropy <- decision_tree |>  set_engine("C5.0")
```

```{r}
# Flujo de trabajo
# pca
workflow_radial_pca <- 
  workflow() |> 
  add_recipe(radial_pca_rec) |> 
  add_model(decision_tree_gini)
```

```{r}
# Grid
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4),
              "min_n" = c(2, 5, 7, 10, 20),
              "cost" = c(0.001, 0.01, 0.1, 0.5, 1, 2))
```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_tree
                
# pca
radial_pca_fit_tune <-
  workflow_radial_pca |> 
  tune_grid(resamples = radial_pca_val,
            grid = grid_tree,
            metrics = metricas)
```

```{r}
# Guardar métricas
metrics_arbol_r1_PCA_corr <- list(radial_pca_fit_tune_saved = radial_pca_fit_tune)

save(metrics_arbol_r1_PCA_corr, file = "./metricas/metrics_arbol_r1_PCA_corr.RData")
```

```{r}
load("./metricas/metrics_arbol_r1_PCA_corr.RData")
radial_pca_fit_tune <- metrics_arbol_r1_PCA_corr$radial_pca_fit_tune_saved

```

**Visualización de métricas de la validación cruzada.**

El gráfico muestra cómo varían las métricas de evaluación (AUC, precisión, sensibilidad y especificidad) en función del parámetro de complejidad del árbol de decisión (profundidad del árbol) y del costo del modelo. Cada línea representa una métrica específica, mientras que los puntos indican los valores medios de cada métrica para diferentes valores de profundidad del árbol. Además, las líneas están codificadas por colores para representar diferentes niveles de profundidad del árbol, lo que permite visualizar cómo la complejidad del modelo afecta el rendimiento del árbol de decisión. Esta visualización ayuda a identificar el conjunto óptimo de hiperparámetros que maximiza el rendimiento del modelo en términos de las métricas seleccionadas.

```{r}
# pca
radial_pca_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + 
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del dataset PCA radial incluyendo Correlation")
```

```{r}
best_models <- collect_metrics(radial_pca_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- radial_pca_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir de PCA radial incluyendo Correlation")


```

```{r}
# Selección del mejor
# pca
best_radial_pca <- radial_pca_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_radial_pca <- 
  workflow_radial_pca |> 
  finalize_workflow(best_radial_pca)

final_radial_pca <- 
  final_wf_radial_pca |> 
  last_fit(radial_pca_split)

```

**Resultados.**

Para validación

```{r}
best_model <- radial_pca_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- radial_pca_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo Árbol de Decisión del PCA radial incluyendo Correlation",
       subtitle = "Modelo 0.001cost-4depth-min_n2",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(radial_pca_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)

```

Resultados para train

```{r}

prob_train_radial_pca<- augment(extract_workflow(final_radial_pca), radial_pca_train)

conf_mat_train <-
  prob_train_radial_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_radial_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_radial_pca  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Resultados para test

```{r}
# pca sin correlation
prob_test_radial_pca_corr <- augment(extract_workflow(final_radial_pca), radial_pca_test)

conf_mat_test <-
  prob_test_radial_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_radial_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_radial_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_radial_pca$demented, prob_train_radial_pca$.pred_TRUE)
roc_obj_test <- roc(prob_test_radial_pca_corr$demented, prob_test_radial_pca_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
results_arbol_r1_PCA_corr <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_r1_PCA_corr, file = "./metricas/results_arbol_r1_PCA_corr.RData")
```

**Visualización del árbol.**

Se visualiza el árbol obtenido para el primer *grid*.

```{r}
# pca
final_radial_pca |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión para PCA radial incluyendo Correlation")
```

#### RADIAL sin Correlation

De nuevo, se hace el mismo procedimiento pero para el PCA sin considerar la variable *Correlation*.

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))

#pca sin correlation
pca_PCA1_corr <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1"), - `correlation-r1`), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

radial_pca_corr <- as.data.frame(pca_PCA1_corr$ind$coord) |> 
  mutate(demented = factor(matriz_total$demented, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# train y test
set.seed(65432)

# pca sin correlation
pca_r1_corr_split <- initial_split(radial_pca_corr, strata = demented, prop = 0.8)
pca_r1_corr_train <- training(pca_r1_corr_split)
pca_r1_corr_test <- testing(pca_r1_corr_split)
```

```{r}
# validation cruzada
set.seed(45678)

# pca sin correlation
pca_r1_corr_val <- vfold_cv(radial_pca_corr, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
# pca sin correlation
pca_r1_corr_rec <- recipe(data = pca_r1_corr_train |> select(- id_img), demented ~ .)
```

```{r}
# Modelo
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))

decision_tree_gini <- decision_tree |>  set_engine("rpart") 
decision_tree_entropy <- decision_tree |>  set_engine("C5.0")
```

```{r}
# Flujo de trabajo
# pca sin correlation
workflow_r1_corr <- 
  workflow() |> 
  add_recipe(pca_r1_corr_rec) |> 
  add_model(decision_tree_gini)
```

```{r}
# Grid
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4),
              "min_n" = c(10, 25, 50, 100),
              "cost" = c(0.001, 0.01, 0.1, 1))
```

```{r}
#| eval: false

# Evaluación validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_tree

# pca sin correlation
radial_pca_corr_fit_tune <-
  workflow_r1_corr |> 
  tune_grid(resamples = pca_r1_corr_val,
            grid = grid_tree,
            metrics = metricas)
```

```{r}
# Guardar métricas
metrics_arbol_r1_PCA <- list(radial_pca_corr_fit_tune_saved = radial_pca_corr_fit_tune)

save(metrics_arbol_r1_PCA, file = "./metricas/metrics_arbol_r1_PCA.RData")
```

```{r}
load("./metricas/metrics_arbol_r1_PCA.RData")
radial_pca_corr_fit_tune <- metrics_arbol_r1_PCA$radial_pca_corr_fit_tune_saved
```

**Visualización de métricas de la validación cruzada.**

```{r}
# pca sin correlation
radial_pca_corr_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del dataset PCA radial")
```

La sensibilidad sigue un patrón menos claro que en el caso anterior.

```{r}
best_models <- collect_metrics(radial_pca_corr_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- radial_pca_corr_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir del PCA radial")


```

```{r}
# Selección del mejor
# pca sin correlation
best_radial_pca_corr <- radial_pca_corr_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_radial_pca_corr <- 
  workflow_r1_corr |> 
  finalize_workflow(best_radial_pca_corr)

final_radial_pca_corr <- 
  final_wf_radial_pca_corr |> 
  last_fit(pca_r1_corr_split)
```

**Resultados.**

Para validación

```{r}
best_model <- radial_pca_corr_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- radial_pca_corr_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo Árbol de Decisión del PCA radial",
       subtitle = "Modelo 0.001cost-3depth-min_n10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")
metrics_val <- collect_metrics(radial_pca_corr_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Resultados para train

```{r}
# pca sin correlation
prob_train_radial_pca_corr <- augment(extract_workflow(final_radial_pca_corr), pca_r1_corr_train)

conf_mat_train <-
  prob_train_radial_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_radial_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_radial_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Resultados para test

```{r}
# pca sin correlation
prob_test_radial_pca_corr <- augment(extract_workflow(final_radial_pca_corr), pca_r1_corr_test)

conf_mat_test <-
  prob_test_radial_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_radial_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_radial_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_radial_pca_corr$demented, prob_train_radial_pca_corr$.pred_TRUE)
roc_obj_test <- roc(prob_test_radial_pca_corr$demented, prob_test_radial_pca_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
results_arbol_r1_PCA <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_r1_PCA, file = "./metricas/results_arbol_r1_PCA.RData")
```

Los resultados del árbol de decisión para el PCA sin incluir la variable *Correlation* muestran un AUC de 0.719, una precisión del 68.3%, una sensibilidad del 82.0% y una especificidad del 54.5%.

El AUC obtenido es un ligeramente mayor que en incluyendo la variable Correlation que era de 0.714. Los valores de la precisión y sensibilidad también aumentan, sobre todo este último, pero el valor de la especificidad disminuye considerablemente. Esto no es beneficioso para los resultados del modelo.

De nuevo, no obtenemos resultados concluyentes sobre si es conveniente o no eliminar esta variable, por lo que de ahora en adelante sí se considerará a la hora de hacer PCA.

**Visualización del árbol.**

```{r}
# tres variables
final_radial_pca_corr |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión del PCA radial")
```

#### TOTAL con Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(-contains("SA"))

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img, -category), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

total_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# train y test
set.seed(65432)

# pca
total_pca_split <- initial_split(total_pca, strata = demented, prop = 0.8)
total_pca_train <- training(total_pca_split)
total_pca_test <- testing(total_pca_split)
```

```{r}
# validation cruzada
set.seed(45678)

# pca
total_pca_val <- vfold_cv(total_pca_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
# pca
total_pca_rec <- recipe(data = total_pca_train |> select(- id_img), demented ~ .) 
```

```{r}
# Modelo 
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))

decision_tree_gini <- decision_tree |>  set_engine("rpart") 
decision_tree_entropy <- decision_tree |>  set_engine("C5.0")
```

```{r}
# Flujo de trabajo
# pca
workflow_total_pca <- 
  workflow() |> 
  add_recipe(total_pca_rec) |> 
  add_model(decision_tree_gini)
```

```{r}
# Grid
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4),
              "min_n" = c(2, 5, 7, 10, 20),
              "cost" = c(0.001, 0.01, 0.1, 0.5, 1, 2))
```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_tree
                
# pca
total_pca_fit_tune <-
  workflow_total_pca |> 
  tune_grid(resamples = total_pca_val,
            grid = grid_tree,
            metrics = metricas)
```

```{r}
#| eval: false
# Guardar métricas
metrics_arbol_PCA_corr_total <- list(total_pca_fit_tune_saved = total_pca_fit_tune)

save(metrics_arbol_PCA_corr_total, file = "./metricas/metrics_arbol_PCA_corr_total.RData")
```

```{r}
load("./metricas/metrics_arbol_PCA_corr_total.RData")
total_pca_fit_tune <- metrics_arbol_PCA_corr_total$total_pca_fit_tune_saved

```

**Visualización de métricas de la validación cruzada.**

El gráfico muestra cómo varían las métricas de evaluación (AUC, precisión, sensibilidad y especificidad) en función del parámetro de complejidad del árbol de decisión (profundidad del árbol) y del costo del modelo. Cada línea representa una métrica específica, mientras que los puntos indican los valores medios de cada métrica para diferentes valores de profundidad del árbol. Además, las líneas están codificadas por colores para representar diferentes niveles de profundidad del árbol, lo que permite visualizar cómo la complejidad del modelo afecta el rendimiento del árbol de decisión. Esta visualización ayuda a identificar el conjunto óptimo de hiperparámetros que maximiza el rendimiento del modelo en términos de las métricas seleccionadas.

```{r}
# pca
total_pca_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + 
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del PCA de todas las direcciones")
```

```{r}
best_models <- collect_metrics(total_pca_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_pca_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir del PCA de todas las direcciones incluyendo Correlation")
```

```{r}
# Selección del mejor
# pca
best_total_pca <- total_pca_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_total_pca <- 
  workflow_total_pca |> 
  finalize_workflow(best_total_pca)

final_total_pca <- 
  final_wf_total_pca |> 
  last_fit(total_pca_split)

```

**Resultados.**

Para validación

```{r}
best_model <- total_pca_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_pca_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo Árbol de Decisión del PCA de todas las direcciones icnluyendo Correlation",
       subtitle = "Modelo 0.001cost-4depth-min_n2",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_pca_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Resultados para train

```{r}

prob_train_total_pca<- augment(extract_workflow(final_total_pca), total_pca_train)

conf_mat_train <-
  prob_train_total_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total_pca  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Resultados para test

```{r}
# pca sin correlation
prob_test_total_pca_corr <- augment(extract_workflow(final_total_pca), total_pca_test)

conf_mat_test <-
  prob_test_total_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total_pca$demented, prob_train_total_pca$.pred_TRUE)
roc_obj_test <- roc(prob_test_total_pca_corr$demented, prob_test_total_pca_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
#| eval: false

results_arbol_total_PCA_corr <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_total_PCA_corr, file = "./metricas/results_arbol_total_PCA_corr.RData")
```

**Visualización del árbol.**

Se visualiza el árbol obtenido para el primer *grid*.

```{r}
# pca
final_radial_pca |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión del PCA de todas las direcciones incluyendo Correlation")
```

#### TOTAL sin Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(-contains("SA"), -contains("correlation"))

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img, -category), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

total_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img)
```

```{r}
# train y test
set.seed(65432)

# pca
total_pca_split <- initial_split(total_pca, strata = demented, prop = 0.8)
total_pca_train <- training(total_pca_split)
total_pca_test <- testing(total_pca_split)
```

```{r}
# validation cruzada
set.seed(45678)

# pca
total_pca_val <- vfold_cv(total_pca_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
# pca
total_pca_rec <- recipe(data = total_pca_train |> select(- id_img), demented ~ .) 
```

```{r}
# Modelo 
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))

decision_tree_gini <- decision_tree |>  set_engine("rpart") 
decision_tree_entropy <- decision_tree |>  set_engine("C5.0")
```

```{r}
# Flujo de trabajo
# pca
workflow_total_pca <- 
  workflow() |> 
  add_recipe(total_pca_rec) |> 
  add_model(decision_tree_gini)
```

```{r}
# Grid
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4),
              "min_n" = c(2, 5, 7, 10, 20),
              "cost" = c(0.001, 0.01, 0.1, 0.5, 1, 2))
```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_tree
                
# pca
total_pca_fit_tune <-
  workflow_total_pca |> 
  tune_grid(resamples = total_pca_val,
            grid = grid_tree,
            metrics = metricas)
```

```{r}
#| eval: false
# Guardar métricas
metrics_arbol_PCA_total <- list(total_pca_fit_tune_saved = total_pca_fit_tune)

save(metrics_arbol_PCA_total, file = "./metricas/metrics_arbol_PCA_total.RData")
```

```{r}
load("./metricas/metrics_arbol_PCA_total.RData")
total_pca_fit_tune <- metrics_arbol_PCA_total$total_pca_fit_tune_saved

```

**Visualización de métricas de la validación cruzada.**

El gráfico muestra cómo varían las métricas de evaluación (AUC, precisión, sensibilidad y especificidad) en función del parámetro de complejidad del árbol de decisión (profundidad del árbol) y del costo del modelo. Cada línea representa una métrica específica, mientras que los puntos indican los valores medios de cada métrica para diferentes valores de profundidad del árbol. Además, las líneas están codificadas por colores para representar diferentes niveles de profundidad del árbol, lo que permite visualizar cómo la complejidad del modelo afecta el rendimiento del árbol de decisión. Esta visualización ayuda a identificar el conjunto óptimo de hiperparámetros que maximiza el rendimiento del modelo en términos de las métricas seleccionadas.

```{r}
# pca
total_pca_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + 
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del PCA de todas las direcciones")
```

```{r}
best_models <- collect_metrics(total_pca_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_pca_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir del PCA de todas las direcciones")
```

```{r}
# Selección del mejor
# pca
best_total_pca <- total_pca_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_total_pca <- 
  workflow_total_pca |> 
  finalize_workflow(best_total_pca)

final_total_pca <- 
  final_wf_total_pca |> 
  last_fit(total_pca_split)

```

**Resultados.**

Para validación

```{r}
best_model <- total_pca_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_pca_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo Árbol de Decisión del PCA todas las direcciones",
       subtitle = "Modelo 0.001cost-3depth-min_n2",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_pca_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Resultados para train

```{r}

prob_train_total_pca<- augment(extract_workflow(final_total_pca), total_pca_train)

conf_mat_train <-
  prob_train_total_pca |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total_pca  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total_pca  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Resultados para test

```{r}
# pca sin correlation
prob_test_total_pca_corr <- augment(extract_workflow(final_total_pca), total_pca_test)

conf_mat_test <-
  prob_test_total_pca_corr |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total_pca_corr  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total_pca_corr  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total_pca$demented, prob_train_total_pca$.pred_TRUE)
roc_obj_test <- roc(prob_test_total_pca_corr$demented, prob_test_total_pca_corr$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black")
plot(roc_obj_test, add = TRUE, col = "red")
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
#| eval: false

results_arbol_total_PCA <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_total_PCA, file = "./metricas/results_arbol_total_PCA.RData")
```

**Visualización del árbol.**

Se visualiza el árbol obtenido para el primer *grid*.

```{r}
# pca
final_radial_pca |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión del PCA de todas las direcciones")
```

### 2.2.3 Clasificación con variables

```{r}
# tres variables
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented")

radial_3 <-
  matriz_total |> 
  select(demented, "contrast-r1", "entropy-r1", "variance-r1") |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE")),
         id_img = matriz_total$id_img) 
```

```{r}
# train y test
set.seed(65432)

# tres variables
radial_3_split <- initial_split(radial_3, strata = demented, prop = 0.8)
radial_3_train <- training(radial_3_split)
radial_3_test <- testing(radial_3_split)
```

```{r}
# validation cruzada
set.seed(45678)

# tres variables
radial_3_val <- vfold_cv(radial_3_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta
# tres variables
radial_3_rec <- recipe(data = radial_3_train |> select(- id_img), demented ~ .) 
```

```{r}
# Modelo
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))

decision_tree_gini <- decision_tree |>  set_engine("rpart") 
decision_tree_entropy <- decision_tree |>  set_engine("C5.0")
```

```{r}
# Flujo de trabajo
# radial
workflow_radial_3 <- 
  workflow() |> 
  add_recipe(radial_3_rec) |> 
  add_model(decision_tree_gini)
```

```{r}
# Grid
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4),
              "min_n" = c(10, 25, 50, 100),
              "cost" = c(0.001, 0.01, 0.1, 1))
```

```{r}
#| eval: false

# Evaluación validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, roc_auc, kap)
  
  # Selección del grid que queremos usar
  grid_select <- grid_tree

# tres variables
radial_3_fit_tune <-
  workflow_radial_3 |> 
  tune_grid(resamples = radial_3_val,
            grid = grid_tree,
            metrics = metricas)
```

```{r}
#| eval: false
# Guardar métricas
metrics_arbol_r1_var <- list(radial_3_fit_tune_saved = radial_3_fit_tune)

save(metrics_arbol_r1_var, file = "./metricas/metrics_arbol_r1_var.RData")
```

```{r}
load("./metricas/metrics_v5_r3_arbol_var.RData")
radial_3_fit_tune <- metrics_v5_r3_arbol_var$radial_3_fit_tune_saved
```

**Visualización de métricas de la validación cruzada.**

```{r}
# guardo los resultados de la validacion
metrics_val_arbol_var <- collect_metrics(radial_3_fit_tune)

# Guardar la tabla como un archivo CSV
write.csv(metrics_val_arbol_var, "./tablas_val/metrics_val_arbol_var.csv", row.names = FALSE)
```

```{r}

# tres variables
radial_3_fit_tune |> 
  collect_metrics() |> 
  mutate(tree_depth = factor(depth)) |> 
  ggplot(aes(cost, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  labs(title = "Comportamiento de las métricas de evaluación",
       subtitle = "del árbol de decisión del dataset de variables: Contrast, Entropy y Variance")
```

```{r}
best_models <- collect_metrics(radial_3_fit_tune)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- radial_3_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(cost, depth, min_n, .estimate) |> 
  mutate(model = glue::glue("{cost}cost-{depth}depth-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos Árbol de Decisión", subtitle = "obtenidos a partir del dataset de variables: Contrast, Entorpy y Variance")
```

Sigue un patrón similar al que se obtenía con PCA, aunque en este caso *tree_depth* con valor 4 para la sensibilidad da mejores resultados.

```{r}
# Selección del mejor
# tres variables
best_radial_3 <- radial_3_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy")

final_wf_radial_3 <- 
  workflow_radial_3 |> 
  finalize_workflow(best_radial_3)

final_radial_3 <- 
  final_wf_radial_3 |> 
  last_fit(radial_3_split)
```

**Resultados.**

Para validación

```{r}
best_model <- radial_3_fit_tune |> 
  select_by_one_std_err(cost, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- radial_3_fit_tune |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de Validación del Mejor Modelo Árbol de Decisión del dataset de variables: Contrast, Entorpy y Variance",
       subtitle = "Modelo 0.001cost-4depth-min_n10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(radial_3_fit_tune)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
# tres variables
prob_train_radial_3 <- augment(extract_workflow(final_radial_3), radial_3_train)

conf_mat_train <-
  prob_train_radial_3 |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_radial_3  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_radial_3  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}
# tres variables
prob_test_radial_3 <- augment(extract_workflow(final_radial_3), radial_3_test)

conf_mat_test <-
  prob_test_radial_3 |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_radial_3  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_radial_3  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_radial_3$demented, prob_train_radial_3$.pred_TRUE)
roc_obj_test <- roc(prob_test_radial_3$demented, prob_test_radial_3$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

Un AUC de 0.699 sugiere que el modelo tiene un rendimiento moderado para distinguir entre las clases positiva y negativa. La precisión toma un valor de 65.5%. La sensibilidad del 81.2% indica que el modelo tiene una buena capacidad para detectar los casos positivos de la clase objetivo. Sin embargo, la especificidad del 49.2% indica que el modelo tiene una capacidad baja para identificar los casos negativos de demencia. Son resultados similares a los del apartado anterior quitando la variable *Correlation* y son considerados peores que los de PCA.

En general el árbol de decisión ha aportado peores resultados que el KNN.

**Visualizar árbol.**

```{r}
# tres variables
final_radial_3 |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 1,
             main = "Árbol de decisión de las variables: Contrast, Entropy y Variance")
```

```{r}
#| eval: false

results_arbol_r1_var <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_arbol_r1_var, file = "./metricas/results_arbol_r1_var.RData")
```

## 2.3 Random Forest

Random Forest es un algoritmo de aprendizaje supervisado utilizado para problemas de clasificación y regresión (clasificación en este caso). Este método se basa en la construcción de múltiples árboles de decisión durante el entrenamiento y luego combina sus predicciones para mejorar la precisión y generalización del modelo. Durante el proceso de construcción de cada árbol, se selecciona un subconjunto aleatorio de características y se utiliza para dividir los nodos, lo que ayuda a reducir el sobreajuste y mejorar la capacidad de generalización del modelo.

Para optimizar el rendimiento del modelo Random Forest, es necesario ajustar ciertos parámetros, por lo que se ha construido un *grid* en el que se han ajustado los siguientes parámetros:

-   **Número de árboles (*trees*):** este parámetro controla la cantidad de árboles de decisión que se construyen en el modelo.

-   **Número de predictores aleatorios (*mtry*):** se selecciona un subconjunto aleatorio de características en cada división de nodo de cada árbol. El parámetro *mtry* controla cuántos predictores se consideran en cada división.

-   **Tamaño mínimo del nodo (*min_n*):** este parámetro determina el tamaño mínimo que un nodo puede tener antes de que no se realicen más divisiones. Un valor más alto de *min_n* produce árboles más pequeños y menos profundos, lo que puede reducir el riesgo de sobreajuste.

Al igual que en los algortimos anteriores se ha hecho una partición, el 80% de las imágenes se usan para entrenar y el 20% como datos de prueba para evaluar el modelo. También se han elegido los hiperparámetros óptimos mediante validación cruzada aplicada en los datos de entrenamiento y finalmente se han obtenido los resultados finales de aplicar el modelo ajustado a los datos de prueba.

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"), -contains("correlation"), -category) |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE")))

```

```{r}
# train y test
set.seed(65432)

total_split <- initial_split(matriz_total, strata = demented, prop = 0.8)
total_train <- training(total_split)
total_test <- testing(total_split)
```

```{r}
# validation cruzada
set.seed(45678)

total_val <- vfold_cv(total_train, strata = demented, v = 5, repeats = 3)
```

```{r}
# Receta

total_rec <- recipe(data = total_train |> select(- id_img), demented ~ .) 
```

```{r}
# Modelo de Random Forest
random_forest_model <- rand_forest(mode = "classification", trees = tune("trees"), mtry = tune("mtry"), min_n = tune("min_n"))

random_forest_rf <- random_forest_model |>  set_engine("randomForest", importance = "impurity") 
random_forest_ranger <- random_forest_model |>  set_engine("ranger", importance = "impurity")
```

```{r}
# Flujo de trabajo

workflow_total <- 
  workflow() |> 
  add_recipe(total_rec) |> 
  add_model(random_forest_ranger)
```

```{r}
# Grid para Random Forest
grid_random_forest <- expand_grid(
  "trees" = c(5, 10, 25, 50),
  "mtry" = c(1, 3, 6),
  "min_n" = c(10, 25, 50, 100))
```

```{r}
#| eval: false

# Evaluación en validación
# definimos algunas variablles:

  # Entrenamos y evaluamos los modelos
  metricas <- metric_set(accuracy, sens, spec, kap, roc_auc)
  
  # Selección del grid que queremos usar
  grid_select <- grid_random_forest
                
# pca
total_fit_random <-
  workflow_total |> 
  tune_grid(resamples = total_val,
            grid = grid_select,
            metrics = metricas)
```

```{r}
# Guardar métricas
metrics_v5_r3_RF_total <- list(total_fit_random_saved = total_fit_random)

save(metrics_v5_r3_RF_total, file = "./metricas/metrics_v5_r3_RF_total.RData")
```

```{r}
load("./metricas/metrics_v5_r3_RF_total.RData")
total_fit_random <- metrics_v5_r3_RF_total$total_fit_random_saved
```

**Gráfico mejores modelos de la validación cruzada.**

```{r}
# guardo los resultados de la validacion
metrics_val_RF_total <- collect_metrics(total_fit_random)

# Guardar la tabla como un archivo CSV
write.csv(metrics_val_RF_total, "./tablas_val/metrics_val_RF_total.csv", row.names = FALSE)
```

```{r}
colores <- c("#8B4513", "#A0522D", "#CD853F", "#59546C", "#D2B48C", "#BC8F8F",              "#5F9EA0",  "#CD5C5C", "#8B0000", "#FF6347", "#FF4500", "#A188A6", "#8A0808",               "#B22222", "#DC143C", "#FF1493", "#C71585", "#DB7093",               "#C2B97F", "#8E5572", "#88665D", "#FFD700", "#FFA500", "#FF8C00",               "#FF7F50", "#FF6347", "#FF4500", "#778DA9", "#CD5C5C", "red")
```

```{r}
best_models <- collect_metrics(total_fit_random)|> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 30, with_ties = FALSE) |> 
  pull(.config)

best_model_metrics <- total_fit_random |> 
  unnest(cols = .metrics) |> 
  filter(.metric == "accuracy") |> 
  filter(.config %in% best_models) |> 
  select(mtry, trees, min_n, .estimate) |> 
  mutate(model = glue::glue("{mtry}mtry-{trees}trees-min_n{min_n}"))


# Paso 1: Calcular la media de .estimate para cada modelo
model_means <- best_model_metrics |> 
  group_by(model) |> 
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE)) |> 
  arrange(desc(mean_estimate))

# Paso 2: Convertir 'model' a un factor con los niveles ordenados según la media
best_model_metrics <- best_model_metrics |> 
  mutate(model = factor(model, levels = model_means$model))

# Paso 3: Crear el gráfico con ggplot2
ggplot(best_model_metrics, aes(x = model, y = .estimate, fill = model, color = model)) +
  geom_boxplot(alpha = 0.5, width = 0.2, size = 1, outlier.shape = NA) +
  gghalves::geom_half_point(side = "l", range_scale = .3, alpha = 0.5, size = 1) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Modelos") +
  scale_fill_manual(values = colores) +
  scale_color_manual(values = colores) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") +
  labs(title = "Los 30 mejores modelos de Random Forest")


```

Para ver cuál ha sido el mejor de los modelos se selecciona el modelo con el mayor valor del hiperparámetro *trees* (número de árboles) cuyo desempeño en precisión (accuracy) está dentro de una desviación estándar del mejor desempeño observado.

El mejor modelo obtenido es el *3mtry-50-min_n10*. Es decir, el mejor modelo se ha dado para un valor de 1 en los predictores aleatorios, construyendo 100 árboles y con un tamaño mínimo del nodo de 9.

```{r}
# Selección del mejor
best_total <- total_fit_random |> 
  select_by_pct_loss(trees, metric = "accuracy")

final_wf_total <- 
  workflow_total |> 
  finalize_workflow(best_total)

final_total <- 
  final_wf_total |> 
  last_fit(total_split)
```

**Importancia de las variables.**

```{r}

final_total |>  
  extract_fit_parsnip() |> 
  vip(num_features = 20)
```

**Resultados.**

Para validación

```{r}
best_model <- total_fit_random |> 
  select_by_one_std_err(trees, metric = "accuracy") |> 
  pull(.config)

best_model_metrics <- total_fit_random |> 
  unnest(cols = .metrics) |> 
  filter(.metric %in% c("accuracy", "kap", "sens", "spec", "roc_auc")) |> 
  filter(.config == best_model) |> 
  select(.estimate, .metric)

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(best_model_metrics, aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas del Mejor Modelo RF",
       subtitle = "Modelo 3mtry-25trees-min_n10",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

metrics_val <- collect_metrics(total_fit_random)|> filter(.config == best_model) |> 
  select(.metric, mean, std_err)
```

Para train

```{r}
prob_train_total <- augment(extract_workflow(final_total), total_train)

conf_mat_train <-
  prob_train_total |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_train |>  summary()

roc_data_train <- prob_train_total  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_train |> autoplot()

auc_train <- prob_train_total  |> roc_auc(truth = demented, .pred_TRUE)
auc_train
```

Para test

```{r}

prob_test_total <- augment(extract_workflow(final_total), total_test)

conf_mat_test <-
  prob_test_total |> 
  conf_mat(truth = demented, estimate = .pred_class)
conf_mat_test |>  summary()

roc_data_test <- prob_test_total  |> roc_curve(truth = demented, .pred_TRUE)
roc_data_test |> autoplot()

auc_test <- prob_test_total  |> roc_auc(truth = demented, .pred_TRUE)
auc_test

# Convertir los datos ROC a un objeto de roc para plotear
roc_obj_train <- roc(prob_train_total$demented, prob_train_total$.pred_TRUE)
roc_obj_test <- roc(prob_test_total$demented, prob_test_total$.pred_TRUE)

# Graficar ambas curvas ROC en el mismo gráfico
plot(roc_obj_train, col = "black", legacy.axes = TRUE)
plot(roc_obj_test, add = TRUE, col = "red", legacy.axes = TRUE)
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lwd = 2)
```

```{r}
results_RF_todas <- list(
  conf_mat_test_saved = conf_mat_test,
  roc_data_test_saved = roc_data_test,
  auc_test_saved = auc_test,
  conf_mat_train_saved = conf_mat_train,
  roc_data_train_saved = roc_data_train,
  auc_train_saved = auc_train,
  metrics_val_saved = metrics_val
)

# Guardar los resultados en un archivo .rdata
save(results_RF_todas, file = "./metricas/results_RF_todas.RData")
```

## 2.4 Regresión Logística Binaria

La regresión logística es un método de clasificacion para predecir una variable dependiente categórica, en este caso binaria (que toma valores 0 o 1) a partir de un conjunto de variables independientes. En este enfoque, se busca modelar la probabilidad de que la variable dependiente tome el valor de 1 dados los valores de las variables independientes. En lugar de utilizar una función lineal directa, la regresión logística emplea una función de enlace, en particular la función de distribución logística, para garantizar que las predicciones estén en el intervalo (0, 1), lo que es esencial para modelar probabilidades.

El principal motivo para elegir la función de distribución logística como función de enlace es su capacidad para interpretar los parámetros del modelo, proporcionando información sobre cómo las variables independientes afectan las probabilidades de ocurrencia del evento de interés a través de los odds ratio.

### 2.4.1 Clasificación con PCA

#### RADIAL con Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))
  

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1")), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

radial_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$demented, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# pca
radial_pca_split <- initial_split(radial_pca, strata = demented, prop = 0.8)
radial_pca_train <- training(radial_pca_split)
radial_pca_test <- testing(radial_pca_split)
```

**Modelo.**

```{r}
modelo_pca1 <- glm(demented ~ Dim.1 + Dim.2 + Dim.3,
data= radial_pca_train, family = binomial)

summary(modelo_pca1)
```

Todas las componentes principales son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_pca1,type = "II")
```

Las componentes principales del PCA que se han utilizado como predictoras están relacionadas de manera significativa con la variable de interés (demented).

**ODDS-ratio**.

Se obtienen los ODDS-ratio de los efectos del modelo. Con ellos se podrá interpretar el efecto de las variables independientes (componentes principales) sobre la dependiente (demencia).

```{r}
exp(coef(modelo_pca1))
```

-   Para Dim.1: Un odds ratio de 0.8514 indica que un aumento de una unidad en Dim.1 (la primera componente principal del PCA), implica una disminución del 14.86% en las posibilidades de tener demencia.

-   Para Dim.2: Un odds ratio de 0.6544 indica que un aumento de una unidad en Dim.2 (la segunda componente principal del PCA), implica una disminución del 34.56% en las posibilidades de tener demencia.

-   Para Dim.3: Un odds ratio de 1.8969 indica que un aumento de una unidad en Dim.3 (la tercera componente principal del PCA), implica un aumento del 89.69% en las posibilidades de tener demencia.

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_pca1, radial_pca_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=radial_pca_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}

cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(radial_pca_train$demented,probs)

curvaROC$auc
plot(curvaROC)
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_pca1, radial_pca_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=radial_pca_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(radial_pca_test$demented,probs_test)

curvaROC_test$auc
plot(curvaROC)
plot(curvaROC_test, add=T, col=2)



```

(Comento solamente los obtenidos con la partición de prueba, vemos que el auc ha disminuido un poco como se esperaba)

Los resultados del modelo de regresión logística muestran una precisión del 67.73% y un coeficiente kappa de 0.3547, indicando una concordancia moderada entre las clasificaciones observadas y las esperadas por azar. La sensibilidad y la especificidad del modelo son del 67.34% y 68.13%, respectivamente, lo que sugiere una capacidad razonable para detectar verdaderos positivos y negativos. El área bajo la curva ROC es del 74.03%, lo que indica una capacidad aceptable para distinguir entre las clases positiva y negativa. En general, el modelo de regresión logística muestra un rendimiento moderado, con margen para mejoras en la sensibilidad y especificidad.

Son resultados muy similares a los que se obtenían en los anteriores modelos, a diferencia de que en este caso no hay tanta diferencia entre los valores de la especificidad y de la sensibilidad.

**Validación cruzada.**

```{r}
dataVCR<-radial_pca_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 3,             
                     repeats = 5,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ Dim.1 + Dim.2 + Dim.3,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

boxplot(vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity")], 
        main = "Resultados de Validación Cruzada",
        ylab = "Métricas de Rendimiento",
        names = c("Exactitud", "AUC", "Sensibilidad", "Especificidad"))

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas del Mejor Modelo Regresión logística con PCA radial incluyendo correlation",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")
```

```{r}

# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(radial_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_r1_PCA_corr <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_r1_PCA_corr, file = "./metricas/results_reg_r1_PCA_corr.RData")
```

En el caso de la validación cruzada realizada, se obtiene la media de las métricas de los modelos. En este caso se aplicaron 5 particiones y 20 repeticiones. En media, se observó un AUC de 0.750, lo que sugiere que el modelo exhibe un buen desempeño en la distinción entre las clases positiva y negativa. Además, se obtuvo una sensibilidad y una especificidad media de aproximadamente 68.01% y 69.93%, respectivamente. Es notable que en este escenario, tanto la sensibilidad como la especificidad están más equilibradas en comparación con otros algoritmos de clasificación utilizados anteriormente.

#### RADIAL sin Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))
  

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img) |>
  select(contains("r1"), - `correlation-r1`), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

radial_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$demented, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# pca
radial_pca_split <- initial_split(radial_pca, strata = demented, prop = 0.8)
radial_pca_train <- training(radial_pca_split)
radial_pca_test <- testing(radial_pca_split)
```

**Modelo.**

```{r}
modelo_pca1 <- glm(demented ~ Dim.1 + Dim.2 + Dim.3,
data= radial_pca_train, family = binomial)

summary(modelo_pca1)
```

Todas las componentes principales son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_pca1,type = "II")
```

Las componentes principales del PCA que se han utilizado como predictoras están relacionadas de manera significativa con la variable de interés (demented).

**ODDS-ratio**.

Se obtienen los ODDS-ratio de los efectos del modelo. Con ellos se podrá interpretar el efecto de las variables independientes (componentes principales) sobre la dependiente (demencia).

```{r}
exp(coef(modelo_pca1))
```

-   Para Dim.1: Un odds ratio de 1.072 indica que un aumento de una unidad en Dim.1 (la primera componente principal del PCA), implica un aumento del 7.18% en las posibilidades de tener demencia.

-   Para Dim.2: Un odds ratio de 0.7107 indica que un aumento de una unidad en Dim.2 (la segunda componente principal del PCA), implica una disminución del 71.07% en las posibilidades de tener demencia.

-   Para Dim.3: Un odds ratio de 2.5143 indica que un aumento de una unidad en Dim.3 (la tercera componente principal del PCA), implica un aumento de 2.51 veces en las posibilidades de tener demencia.

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_pca1, radial_pca_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=radial_pca_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}
cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(radial_pca_train$demented, probs)
curvaROC$auc
plot(curvaROC)
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_pca1, radial_pca_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=radial_pca_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(radial_pca_test$demented,probs_test)

curvaROC_test$auc
plot(curvaROC)
plot(curvaROC_test, add=T, col=2)

```

Los resultados del modelo de regresión logística muestran una precisión del 68.52% y un coeficiente kappa de 0.3703, indicando una concordancia moderada entre las clasificaciones observadas y las esperadas por azar. La sensibilidad y la especificidad del modelo son del 69.06% y 67.97%, respectivamente, lo que sugiere una capacidad razonable para detectar verdaderos positivos y negativos. El área bajo la curva ROC es del 74.17%, lo que indica una capacidad aceptable para distinguir entre las clases positiva y negativa. En general, el modelo de regresión logística muestra un rendimiento moderado, con margen para mejoras en la sensibilidad y especificidad.

**Validación cruzada.**

```{r}
dataVCR<-radial_pca_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 5,             
                     repeats = 3,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ Dim.1 + Dim.2 + Dim.3,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

boxplot(vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")], 
        main = "Resultados de Validación Cruzada",
        ylab = "Métricas de Rendimiento",
        names = c("Exactitud", "AUC", "Sensibilidad", "Especificidad", "Kappa"))

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas del Mejor Modelo Regresión logística con PCA radial",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

```

En el caso de la validación cruzada realizada, se obtiene la media de las métricas de los modelos. En este caso se aplicaron 5 particiones y 20 repeticiones. En media, se observó un AUC de 0.7518, lo que sugiere que el modelo exhibe un buen desempeño en la distinción entre las clases positiva y negativa. Además, se obtuvo una sensibilidad y una especificidad media de aproximadamente 70.03% y 69.47%, respectivamente. Es notable que en este escenario, tanto la sensibilidad como la especificidad están más equilibradas en comparación con otros algoritmos de clasificación utilizados anteriormente.

Los resultados han mejorado respecto al PCA con *Correlation*.

```{r}

# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(radial_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_r1_PCA <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_r1_PCA, file = "./metricas/results_reg_r1_PCA.RData")
```

#### TOTAL con Correlation

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(-contains("SA"))
  

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img, -category), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

total_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# pca
total_pca_split <- initial_split(total_pca, strata = demented, prop = 0.8)
total_pca_train <- training(total_pca_split)
total_pca_test <- testing(total_pca_split)
```

**Modelo.**

```{r}
modelo_pca1 <- glm(demented ~ Dim.1 + Dim.2 + Dim.3,
data= total_pca_train, family = binomial)

summary(modelo_pca1)
```

Todas las componentes principales son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_pca1,type = "II")
```

Las componentes principales del PCA que se han utilizado como predictoras están relacionadas de manera significativa con la variable de interés (demented).

**ODDS-ratio**.

Se obtienen los ODDS-ratio de los efectos del modelo. Con ellos se podrá interpretar el efecto de las variables independientes (componentes principales) sobre la dependiente (demencia).

```{r}
exp(coef(modelo_pca1))
```

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_pca1, total_pca_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=total_pca_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}
cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(total_pca_train$demented, probs)
curvaROC$auc
plot(curvaROC)
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_pca1, total_pca_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=total_pca_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(total_pca_test$demented,probs_test)

curvaROC_test$auc
plot(curvaROC)
plot(curvaROC_test, add=T, col=2)

```

**Validación cruzada.**

```{r}
dataVCR<-total_pca_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 5,             
                     repeats = 3,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ Dim.1 + Dim.2 + Dim.3,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

boxplot(vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")], 
        main = "Resultados de Validación Cruzada",
        ylab = "Métricas de Rendimiento",
        names = c("Exactitud", "AUC", "Sensibilidad", "Especificidad", "Kappa"))

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas del Mejor Modelo Regresión logística con PCA de todas las direcciones incluyendo Correlation",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

```

Los resultados han mejorado respecto al PCA con *Correlation*.

```{r}

# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(total_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_total_PCA_corr <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_total_PCA_corr, file = "./metricas/results_reg_total_PCA_corr.RData")
```

#### TOTAL sin Correlation

```{r}
#| message: false

#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(category = category != "non_demented") |> 
  select(-contains("SA"), -contains("correlation"))

# pca

pca_PCA1 <-
  PCA(matriz_total |> 
  select(-id_img, -category), 
  scale.unit = TRUE, ncp = 3, graph = FALSE)

total_pca <- as.data.frame(pca_PCA1$ind$coord) |> 
  mutate(demented = factor(matriz_total$category, levels = c("TRUE", "FALSE")),id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# pca
total_pca_split <- initial_split(total_pca, strata = demented, prop = 0.8)
total_pca_train <- training(total_pca_split)
total_pca_test <- testing(total_pca_split)
```

**Modelo.**

```{r}
modelo_pca1 <- glm(demented ~ Dim.1 + Dim.2 + Dim.3,
data= total_pca_train, family = binomial)

summary(modelo_pca1)
```

Todas las componentes principales son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_pca1,type = "II")
```

Las componentes principales del PCA que se han utilizado como predictoras están relacionadas de manera significativa con la variable de interés (demented).

**ODDS-ratio**.

Se obtienen los ODDS-ratio de los efectos del modelo. Con ellos se podrá interpretar el efecto de las variables independientes (componentes principales) sobre la dependiente (demencia).

```{r}
exp(coef(modelo_pca1))
```

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_pca1, total_pca_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=total_pca_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}
cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(total_pca_train$demented, probs)
curvaROC$auc
plot(curvaROC)
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_pca1, total_pca_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=total_pca_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(total_pca_test$demented,probs_test)

curvaROC_test$auc
plot(curvaROC)
plot(curvaROC_test, add=T, col=2)

```

**Validación cruzada.**

```{r}
dataVCR<-total_pca_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 5,             
                     repeats = 3,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ Dim.1 + Dim.2 + Dim.3,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

boxplot(vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")], 
        main = "Resultados de Validación Cruzada",
        ylab = "Métricas de Rendimiento",
        names = c("Exactitud", "AUC", "Sensibilidad", "Especificidad", "Kappa"))

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas del Mejor Modelo Regresión logística con PCA de todas las direcciones",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")

```

Los resultados han mejorado respecto al PCA con *Correlation*.

```{r}

# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(total_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_total_PCA <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_total_PCA, file = "./metricas/results_reg_total_PCA.RData")
```

### 2.4.2 Clasificación con variables

#### Entropy, Variance, Contrast

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))

radial_3 <-
  matriz_total |> 
  select(demented, contains("r1")) |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE")),
         id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# tres variables
radial_3_split <- initial_split(radial_3, strata = demented, prop = 0.8)
radial_3_train <- training(radial_3_split)
radial_3_test <- testing(radial_3_split)
```

**Modelo.**

```{r}
modelo_variables <- glm(demented ~ `contrast-r1` + `entropy-r1` + `variance-r1`,
data= radial_3_train, family = binomial)

summary(modelo_variables)
```

Todas las variables son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_variables,type = "II")
```

Todos son significativos.

**ODDS-ratio**.

Se obtienen los ODDS-ratio de los efectos del modelo. Con ellos se podrá interpretar el efecto de las variables independientes (componentes principales) sobre la dependiente (demencia).

```{r}
exp(coef(modelo_variables))
```

-   Para *Contrast*: Un odds ratio de 1.013 indica que un aumento de una unidad en la variable *Contrast*, implica un aumento del 1.3% en las posibilidades de tener demencia.

-   Para *Entropy*: Un odds ratio de 0.0886 indica que un aumento de una unidad en la variable *Entropy*, implica una disminución del 91.14% en las posibilidades de tener demencia.

-   Para *Variance*: Un odds ratio de 0.999 indica que un aumento de una unidad en la variable *Variance*, implica una disminución del 1.1% en las posibilidades de tener demencia.

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_variables, radial_3_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=radial_3_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}
cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(radial_3_train$demented, probs)
curvaROC$auc
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_variables, radial_3_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=radial_3_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(radial_3_test$demented,probs_test)
curvaROC_test$auc

plot(curvaROC)
plot(curvaROC_test, add=T, col=2)
```

Los resultados del modelo de regresión logística muestran una precisión del 66.72% y un coeficiente kappa de 0.3464, indicando una concordancia moderada entre las clasificaciones observadas y las esperadas por azar. La sensibilidad y la especificidad del modelo son del 67.19% y 66.25%, respectivamente, lo que sugiere una capacidad razonable para detectar verdaderos positivos y negativos. El área bajo la curva ROC es del 72.64%, lo que indica una capacidad aceptable para distinguir entre las clases positiva y negativa. En general, el modelo de regresión logística muestra un rendimiento moderado, con margen para mejoras en la sensibilidad y especificidad.

Son resultados muy similares a los que se obtenían en el modelo del PCA.

**Validación cruzada.**

```{r}
dataVCR<-radial_3_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 5,             
                     repeats = 3,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ `contrast-r1` + `entropy-r1` + `variance-r1`,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de la validación cruzada de Regresión logística con variables: Contrast, Entropy y Variance",
       subtitle = "Entropy, Variance y Contrast",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")
```

En el caso de la validación cruzada realizada, se observó un AUC de 0.7356, lo que sugiere que el modelo exhibe un buen desempeño en la distinción entre las clases positiva y negativa. Además, se obtuvo una sensibilidad y una especificidad de aproximadamente 66.81% y 68.59%, respectivamente. Es notable que en este escenario, tanto la sensibilidad como la especificidad están más equilibradas en comparación con otros algoritmos de clasificación utilizados anteriormente.

Los resultados son casi iguales a los del caso anterior.

```{r}
# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(radial_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_r1_var <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_r1_var, file = "./metricas/results_reg_r1_var.RData")
```

#### ASM, Mean y Contrast

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))

radial_3 <-
  matriz_total |> 
  select(demented, contains("r1")) |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE")),
         id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# tres variables
radial_3_split <- initial_split(radial_3, strata = demented, prop = 0.8)
radial_3_train <- training(radial_3_split)
radial_3_test <- testing(radial_3_split)
```

**Modelo.**

```{r}
modelo_variables2 <- glm(demented ~ `contrast-r1` + `ASM-r1` + `mean-r1`,
data= radial_3_train, family = binomial)

summary(modelo_variables2)
```

Todas las variables son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_variables2,type = "II")
```

Todos son significativos.

**ODDS-ratio**.

```{r}
exp(coef(modelo_variables2))
```

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_variables2, radial_3_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=radial_3_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}
cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(radial_3_train$demented, probs)
curvaROC$auc
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_variables2, radial_3_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=radial_3_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(radial_3_test$demented,probs_test)
curvaROC_test$auc

plot(curvaROC)
plot(curvaROC_test, add=T, col=2)
```

**Validación cruzada.**

```{r}
dataVCR<-radial_3_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 5,             
                     repeats = 3,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ `contrast-r1` + `ASM-r1` + `mean-r1`,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de la validación cruzada de Regresión logística con variables: Contrast, ASM y Mean",
       subtitle = "ASM, Mean y Contrast",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")
```

```{r}
# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(radial_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_r1_var2 <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_r1_var2, file = "./metricas/results_reg_r1_var2.RData")
```

### AIC/BIC

#### RADIAL con Correlation

```{r}
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"), -id_img)

radial_corr <-
  matriz_total |> 
  select(demented, contains("r1")) |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE"))) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))

# train y test
set.seed(65432)

# tres variables
data_split <- initial_split(radial_corr, strata = demented, prop = 0.8)
data_train <- training(data_split)
data_test <- testing(data_split)
```

```{r}
null<-glm(demented~1,data=data_train,family=binomial)
full<-glm(demented~.,data=data_train,family=binomial)
```

```{r}
modeloStepBIC<-stats::step(null, scope=list(lower=null, upper=full), direction="both",
k=log(nrow(data_train)), trace=F)

modeloStepAIC<-stats::step(null, scope=list(lower=null, upper=full), direction="both",trace=F)

modeloForwBIC<-stats::step(null, scope=list(lower=null, upper=full), direction="forward",
k=log(nrow(data_train)),trace=F)

modeloForwAIC<-stats::step(null, scope=list(lower=null, upper=full), direction="forward",trace=F)

modeloBackBIC<-stats::step(full, scope=list(lower=null, upper=full), direction="backward",
k=log(nrow(data_train)),trace=F)

modeloBackAIC<-stats::step(full, scope=list(lower=null, upper=full), direction="backward",trace=F)
```

```{r}
modelos<-list(modeloStepBIC,modeloStepAIC,modeloForwBIC,modeloForwAIC,modeloBackBIC,
modeloBackAIC)
sapply(modelos,function(x) formula(x))
```

```{r}
sapply(modelos,function(x) x$rank)
```

Da lugar a tres modelos diferentes. Vamos a comparar esos tres modelos con el de las dos formas de incluir las variables:

```{r}
dataVCR<-radial_corr
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
modelos<-list(modelo_variables, modelo_variables2, modeloStepBIC, modeloStepAIC, modeloForwAIC)
vcrTodosModelos<-list()
for (i in 1:length(modelos)){
set.seed(12345)
vcr<-train(formula(modelos[[i]]), data = dataVCR,
method = "glm", family="binomial",
trControl = trainControl(method="repeatedcv", number=5, repeats=3,
summaryFunction=multiClassSummary, classProbs=TRUE,
savePredictions = TRUE)
)
vcrTodosModelos[[i]]<-vcr
}
bwplot(resamples(vcrTodosModelos), metric=c("Accuracy", "AUC", "Kappa", "Sensitivity", "Specificity"),
scales = list(x = list(relation = "free")))

```

```{r}
summary(resamples(vcrTodosModelos), metric=c("AUC", "Kappa", "Accuracy", "Sens"))
```

```{r}
sapply(modelos,function(x) x$rank)
```

Concluimos que de los modelos que se habían considerado primeramente es mejor el modelo 2, el que incluía las variables ASM, Mean y Variance. Los tres modelos considerados en el análisis AIC/BIC son muy similares. Se va a comprobar si se obtienen los mismo resultados sin incluir la variables Correlation.

#### RADIAL sin Correlation

```{r}
matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"), -id_img)

radial_corr <-
  matriz_total |> 
  select(demented, contains("r1"), -contains("correlation")) |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE"))) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))

# train y test
set.seed(65432)

# tres variables
data_split <- initial_split(radial_corr, strata = demented, prop = 0.8)
data_train <- training(data_split)
data_test <- testing(data_split)
```

```{r}
null<-glm(demented~1,data=data_train,family=binomial)
full<-glm(demented~.,data=data_train,family=binomial)
```

```{r}
modeloStepBIC<-stats::step(null, scope=list(lower=null, upper=full), direction="both",
k=log(nrow(data_train)), trace=F)

modeloStepAIC<-stats::step(null, scope=list(lower=null, upper=full), direction="both",trace=F)

modeloForwBIC<-stats::step(null, scope=list(lower=null, upper=full), direction="forward",
k=log(nrow(data_train)),trace=F)

modeloForwAIC<-stats::step(null, scope=list(lower=null, upper=full), direction="forward",trace=F)

modeloBackBIC<-stats::step(full, scope=list(lower=null, upper=full), direction="backward",
k=log(nrow(data_train)),trace=F)

modeloBackAIC<-stats::step(full, scope=list(lower=null, upper=full), direction="backward",trace=F)
```

```{r}
modelos<-list(modeloStepBIC,modeloStepAIC,modeloForwBIC,modeloForwAIC,modeloBackBIC,
modeloBackAIC)
sapply(modelos,function(x) formula(x))
```

```{r}
sapply(modelos,function(x) x$rank)
```

En este caso también se tienen tres modelos diferentes. Vamos a comparar esos tres modelos con el de las dos formas de incluir las variables:

```{r}
dataVCR<-radial_corr
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
modelos<-list(modelo_variables, modelo_variables2, modeloStepBIC, modeloStepAIC, modeloForwAIC)
vcrTodosModelos<-list()
for (i in 1:length(modelos)){
set.seed(12345)
vcr<-train(formula(modelos[[i]]), data = dataVCR,
method = "glm", family="binomial",
trControl = trainControl(method="repeatedcv", number=5, repeats=3,
summaryFunction=multiClassSummary, classProbs=TRUE,
savePredictions = TRUE)
)
vcrTodosModelos[[i]]<-vcr
}
bwplot(resamples(vcrTodosModelos), metric=c("Accuracy", "AUC", "Kappa", "Sensitivity", "Specificity"),
scales = list(x = list(relation = "free")))

```

```{r}
summary(resamples(vcrTodosModelos), metric=c("Accuracy", "AUC", "Kappa","Specificity"))
```

```{r}
sapply(modelos,function(x) x$rank)
```

Concluimos que de los modelos que se habían considerado primeramente es mejor el modelo 2, el que incluía las variables ASM, Mean y Variance. En este caso también se solapa con los otros modelos.

Es curioso porque uno de los modelos incluye una variable de cada grupo excepto variance y mean que eran las variables entre las que se dudaba para seleccionar de su grupo:

demented \~ `mean-r1` + `ASM-r1` + `variance-r1` + `homogeneity-r1`

Al ser un modelo similar y el más sencillo de los obtenidos en el AIC/BIC se va a estudiar ese caso:

#### ASM, Mean, Variance, Homogeneity

```{r}
#| message: false

matriz_total <- read_csv("matrices-glcm/matriz_total.csv") |> 
  rename_with(~ str_remove(., "^glcm_"), starts_with("glcm_")) |> 
  mutate(demented = category != "non_demented") |> 
  select(-contains("SA"))

radial_3 <-
  matriz_total |> 
  select(demented, contains("r1")) |> 
  mutate(demented = factor(demented, levels = c("TRUE", "FALSE")),
         id_img = matriz_total$id_img) |> 
  mutate(demented = factor(ifelse(demented == "TRUE", 1, 0), levels = c(0, 1)))
```

```{r}
# train y test
set.seed(65432)

# tres variables
radial_3_split <- initial_split(radial_3, strata = demented, prop = 0.8)
radial_3_train <- training(radial_3_split)
radial_3_test <- testing(radial_3_split)
```

**Modelo.**

```{r}
modelo_variables3 <- glm(demented ~ `homogeneity-r1` + `ASM-r1` + `mean-r1` + `variance-r1`, data= radial_3_train, family = binomial)

summary(modelo_variables3)
```

Todas las variables son significativas.

**Test ANOVA.**

A continuación se realiza un test ANOVA.

```{r}
Anova(modelo_variables3,type = "II")
```

Todos son significativos.

**ODDS-ratio**.

```{r}
exp(coef(modelo_variables3))
```

**Evaluación del modelo.**

-   Resultados con la partición de entrenamiento

```{r}
probs <-predict(modelo_variables3, radial_3_train, type="response")


cm<-confusionMatrix(data=as.factor(ifelse(probs>=0.5, 1 ,0)),reference=radial_3_train$demented, positive = "1")

```

```{r}
# MATRIZ CONFUSION
cm$table
```

Se calculan los estimadores:

```{r}
cm$overall[1:2]

cm$byClass[1:2]

curvaROC<-roc(radial_3_train$demented, probs)
curvaROC$auc
```

-   Resultados con la partición de prueba:

```{r}
probs_test <-predict(modelo_variables, radial_3_test, type="response")

cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.5,1,0)),reference=radial_3_test$demented, positive = "1")

cm_test$table
```

Se calculan los estimadores

```{r}
cm_test$overall[1:2]

cm_test$byClass[1:2]

curvaROC_test<-roc(radial_3_test$demented,probs_test)
curvaROC_test$auc

plot(curvaROC)
plot(curvaROC_test, add=T, col=2)
```

**Validación cruzada.**

```{r}
dataVCR<-radial_3_train
dataVCR$demented<-factor(dataVCR$demented, levels=rev(levels(dataVCR$demented)), labels=c("Si","No"))
```

```{r}
# Define el control de entrenamiento para la validación cruzada
ctrl <- trainControl(method = "repeatedcv",    
                     number = 5,             
                     repeats = 3,              
                     summaryFunction = multiClassSummary,  
                     classProbs = TRUE,        
                     savePredictions = TRUE)   

# Entrena el modelo usando train()
vcr <- train(demented ~ `homogeneity-r1` + `ASM-r1` + `mean-r1` + `variance-r1`,  
                     data = dataVCR,           
                     method = "glm",                    
                     family = binomial,                 
                     trControl = ctrl)                  

```

```{r}
vcr$results[c(3,5,6,8,9,17,19,20,22,23)]

summary(vcr$resample[c(2,4,5,7,8)])

confusionMatrix(vcr)

metrics <- vcr$resample[, c("Accuracy", "AUC", "Sensitivity", "Specificity", "Kappa")]

# Convertimos los datos al formato largo
metrics_long <- metrics |> 
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

colores_box <- c( "#59546C", "#D2B48C", "#BC8F8F", "#5F9EA0",  "#CD5C5C")

ggplot(metrics_long, aes(x = Metric, y = Value, fill = Metric, color = Metric)) +
  geom_boxplot(alpha = 0.4, width = 0.65, size = 0.7,
               outlier.shape = NA) +
  geom_jitter(alpha = 0.8, size = 1.5) +
  scale_fill_manual(values = colores_box) +
  scale_color_manual(values = colores_box) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Métricas de la validación cruzada de Regresión logística con las variables: Homogeneity, ASM, Mean y Variance",
       subtitle = "ASM, Mean, Variance y Homogeneity",
       x = "Métrica",
       y = "Valor",
       fill = "Métrica")
```

```{r}
# train
tabla1 <- cm$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

curvaROC<-roc(radial_pca_train$demented, probs)

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_train <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC$auc)),
         datos = "train") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

# test
tabla1 <- cm_test$overall[1:2] |> as_tibble() |> 
  mutate(metric = c("accuracy", "kap"))

tabla2 <- cm_test$byClass[1:2] |> as_tibble() |> 
  mutate(metric = c("sens", "spec"))

tabla3 <- bind_rows(tabla1, tabla2) 

tabla_final_test <- tabla3 |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  mutate(auc = as.numeric(gsub("Area under the curve: ", "", curvaROC_test$auc)),
         datos = "test") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))


# val
tabla1 <- vcr$results[c(3,5,6,8,9)] |> 
  rename(auc = AUC,
         accuracy = Accuracy,
         kap = Kappa,
         sens = Sensitivity,
         spec = Specificity) |> 
  mutate(datos = "validación") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla2 <- vcr$results[c(17,19,20,22,23)] |> 
  rename(auc = AUCSD,
         accuracy = AccuracySD,
         kap = KappaSD,
         sens = SensitivitySD,
         spec = SpecificitySD) |> 
  mutate(datos = "std error") |> 
  select(datos, accuracy, auc, sens, spec, kap) |> 
  mutate(across(accuracy:kap, ~ round(.x, 3)))

tabla_final_val <- bind_rows(tabla1, tabla2)

results_reg_r1_var3 <- bind_rows(tabla_final_val, tabla_final_train, tabla_final_test)
```

```{r}
#| eval: false

# Guardar los resultados en un archivo .rdata
save(results_reg_r1_var3, file = "./metricas/results_reg_r1_var3.RData")
```

No nos coje variable del primer grupo porque la regresion logistica se queda corta a la hora de ver la relacion entre las variables y la categoria. Con los modelos no parametricos sí que podemos ver esa relacion
